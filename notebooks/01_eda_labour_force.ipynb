{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03b1bd62",
   "metadata": {},
   "source": [
    "# 🇨🇱 Análisis Exploratorio de la Fuerza de Trabajo en Chile\n",
    "\n",
    "## Análisis Avanzado de Datos del INE\n",
    "\n",
    "**Autor:** Bruno San Martin  \n",
    "**Fecha:** Julio 2025  \n",
    "**Fuente:** Instituto Nacional de Estadísticas (INE) - Chile  \n",
    "\n",
    "---\n",
    "\n",
    "### 📊 Objetivos del Análisis\n",
    "\n",
    "1. **Exploración de Datos**: Entender la estructura y calidad de los datos de fuerza laboral\n",
    "2. **Análisis Temporal**: Identificar tendencias y patrones a lo largo del tiempo\n",
    "3. **Análisis Regional**: Comparar indicadores entre diferentes regiones de Chile\n",
    "4. **Análisis por Género**: Examinar diferencias en participación laboral por sexo\n",
    "5. **Insights Económicos**: Generar conclusiones actionables para stakeholders\n",
    "\n",
    "### 🎯 Metodología\n",
    "\n",
    "- **Clean Code**: Código limpio y bien documentado\n",
    "- **SOLID Principles**: Arquitectura modular y escalable  \n",
    "- **Data Science Best Practices**: Validación, logging y reproducibilidad\n",
    "- **Professional Visualizations**: Gráficos de calidad ejecutiva"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bf6a3d",
   "metadata": {},
   "source": [
    "## 1. 🛠️ Configuración del Entorno y Librerías\n",
    "\n",
    "Configuración inicial del entorno de trabajo, importación de librerías necesarias y configuración de rutas del proyecto siguiendo arquitectura modular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78971f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración del entorno\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# Añadir el directorio src al path para importar nuestros módulos\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.append(str(project_root / \"src\"))\n",
    "\n",
    "# Suprimir warnings innecesarios\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"📁 Directorio del proyecto: {project_root}\")\n",
    "print(f\"🐍 Python path configurado correctamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283ec6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerías principales para Data Science\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Librerías para análisis estadístico\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Librerías del proyecto (arquitectura modular)\n",
    "try:\n",
    "    from etl.data_processor import LabourForceProcessor\n",
    "    from visualization.charts import LabourForceCharts, StatisticalCharts\n",
    "    from utils.validators import LabourForceValidator\n",
    "    from utils.logger_config import get_logger\n",
    "    print(\"✅ Módulos del proyecto importados correctamente\")\n",
    "except ImportError as e:\n",
    "    print(f\"⚠️  Error importando módulos del proyecto: {e}\")\n",
    "    print(\"📋 Continuaremos con librerías estándar\")\n",
    "\n",
    "# Configuración de visualizaciones\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "# Configuración de plotly\n",
    "import plotly.io as pio\n",
    "pio.templates.default = \"plotly_white\"\n",
    "\n",
    "print(\"📊 Configuración de visualización completada\")\n",
    "print(\"🎯 Entorno listo para análisis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965aaa24",
   "metadata": {},
   "source": [
    "## 2. 📥 Extracción y Organización de Datos del INE\n",
    "\n",
    "Carga y exploración inicial de los datos oficiales de la Fuerza de Trabajo proporcionados por el Instituto Nacional de Estadísticas (INE) de Chile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2dd0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir rutas de datos\n",
    "raw_data_path = project_root / \"data\" / \"raw\"\n",
    "processed_data_path = project_root / \"data\" / \"processed\"\n",
    "output_data_path = project_root / \"data\" / \"outputs\"\n",
    "\n",
    "# Crear directorios si no existen\n",
    "for path in [raw_data_path, processed_data_path, output_data_path]:\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"📂 Directorio de datos crudos: {raw_data_path}\")\n",
    "print(f\"📂 Directorio de datos procesados: {processed_data_path}\")\n",
    "print(f\"📂 Directorio de salida: {output_data_path}\")\n",
    "\n",
    "# Listar archivos disponibles en data/raw\n",
    "raw_files = list(raw_data_path.glob(\"*.csv\"))\n",
    "print(f\"\\n📋 Archivos CSV disponibles: {len(raw_files)}\")\n",
    "for file in raw_files:\n",
    "    file_size = file.stat().st_size / (1024*1024)  # MB\n",
    "    print(f\"   📄 {file.name} ({file_size:.2f} MB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16fc529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar datos del INE\n",
    "if raw_files:\n",
    "    # Tomar el primer archivo CSV encontrado\n",
    "    data_file = raw_files[0]\n",
    "    print(f\"📊 Cargando datos desde: {data_file.name}\")\n",
    "    \n",
    "    # Cargar datos\n",
    "    try:\n",
    "        df_raw = pd.read_csv(data_file, encoding='utf-8')\n",
    "        print(f\"✅ Datos cargados exitosamente\")\n",
    "        print(f\"📐 Dimensiones: {df_raw.shape}\")\n",
    "        \n",
    "    except UnicodeDecodeError:\n",
    "        # Intentar con encoding alternativo\n",
    "        df_raw = pd.read_csv(data_file, encoding='latin-1')\n",
    "        print(f\"✅ Datos cargados con encoding latin-1\")\n",
    "        print(f\"📐 Dimensiones: {df_raw.shape}\")\n",
    "        \n",
    "else:\n",
    "    print(\"❌ No se encontraron archivos CSV en data/raw\")\n",
    "    print(\"📋 Por favor, coloca el archivo de datos del INE en la carpeta data/raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1566876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploración inicial de los datos\n",
    "if 'df_raw' in locals():\n",
    "    print(\"🔍 EXPLORACIÓN INICIAL DE DATOS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Información básica\n",
    "    print(f\"📊 Forma del dataset: {df_raw.shape}\")\n",
    "    print(f\"📋 Columnas: {list(df_raw.columns)}\")\n",
    "    \n",
    "    # Mostrar primeras filas\n",
    "    print(\"\\n📖 Primeras 5 filas:\")\n",
    "    display(df_raw.head())\n",
    "    \n",
    "    # Información de tipos de datos\n",
    "    print(\"\\n🔢 Información de tipos de datos:\")\n",
    "    display(df_raw.info())\n",
    "    \n",
    "    # Estadísticas descriptivas\n",
    "    print(\"\\n📈 Estadísticas descriptivas:\")\n",
    "    display(df_raw.describe(include='all'))\n",
    "    \n",
    "    # Valores únicos en columnas categóricas\n",
    "    print(\"\\n🏷️ Valores únicos en columnas principales:\")\n",
    "    categorical_cols = df_raw.select_dtypes(include=['object']).columns\n",
    "    for col in categorical_cols[:5]:  # Mostrar solo las primeras 5\n",
    "        unique_count = df_raw[col].nunique()\n",
    "        print(f\"   {col}: {unique_count} valores únicos\")\n",
    "        if unique_count <= 10:\n",
    "            print(f\"      Valores: {list(df_raw[col].unique())}\")\n",
    "        else:\n",
    "            print(f\"      Algunos valores: {list(df_raw[col].unique()[:5])}...\")\n",
    "else:\n",
    "    print(\"⚠️ Datos no disponibles para exploración\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee6783d",
   "metadata": {},
   "source": [
    "## 3. 🧹 Procesamiento y Limpieza de Datos (ETL)\n",
    "\n",
    "Aplicación de técnicas de ETL (Extract, Transform, Load) para limpiar, estandarizar y transformar los datos crudos del INE en un formato óptimo para análisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152c7056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validación de datos usando nuestro módulo personalizado\n",
    "if 'df_raw' in locals():\n",
    "    print(\"🔍 VALIDACIÓN DE CALIDAD DE DATOS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Crear validador\n",
    "    try:\n",
    "        validator = LabourForceValidator()\n",
    "        \n",
    "        # Ejecutar validaciones\n",
    "        validation_results = validator.validate_labour_force_data(df_raw)\n",
    "        \n",
    "        # Mostrar reporte\n",
    "        print(\"\\n📋 REPORTE DE VALIDACIÓN:\")\n",
    "        report = validator.get_validation_report()\n",
    "        print(report)\n",
    "        \n",
    "    except NameError:\n",
    "        print(\"⚠️ Usando validación básica (módulo personalizado no disponible)\")\n",
    "        \n",
    "        # Validación básica manual\n",
    "        print(f\"✅ Datos no vacíos: {not df_raw.empty}\")\n",
    "        print(f\"✅ Sin filas completamente vacías: {df_raw.dropna(how='all').shape[0] == df_raw.shape[0]}\")\n",
    "        print(f\"📊 Valores faltantes por columna:\")\n",
    "        missing_data = df_raw.isnull().sum()\n",
    "        missing_data = missing_data[missing_data > 0]\n",
    "        if len(missing_data) == 0:\n",
    "            print(\"   🎉 No hay valores faltantes\")\n",
    "        else:\n",
    "            for col, count in missing_data.items():\n",
    "                percentage = (count / len(df_raw)) * 100\n",
    "                print(f\"   {col}: {count} ({percentage:.2f}%)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9fafc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procesamiento de datos usando arquitectura modular\n",
    "if 'df_raw' in locals():\n",
    "    print(\"🔄 PROCESAMIENTO DE DATOS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    try:\n",
    "        # Usar procesador personalizado\n",
    "        processor = LabourForceProcessor()\n",
    "        \n",
    "        # Procesar datos\n",
    "        print(\"📊 Aplicando transformaciones...\")\n",
    "        df_processed = processor.transformer.transform(df_raw)\n",
    "        \n",
    "        print(f\"✅ Procesamiento completado\")\n",
    "        print(f\"📐 Datos originales: {df_raw.shape}\")\n",
    "        print(f\"📐 Datos procesados: {df_processed.shape}\")\n",
    "        \n",
    "        # Generar estadísticas\n",
    "        stats = processor.get_summary_statistics(df_processed)\n",
    "        print(f\"\\n📈 ESTADÍSTICAS DEL PROCESAMIENTO:\")\n",
    "        print(f\"   📊 Total de registros: {stats['total_records']:,}\")\n",
    "        print(f\"   📅 Rango de fechas: {stats['date_range']['min']} a {stats['date_range']['max']}\")\n",
    "        print(f\"   🌍 Regiones: {stats['regions_count']}\")\n",
    "        print(f\"   📆 Años: {stats['years_count']}\")\n",
    "        print(f\"   ❓ Valores faltantes: {stats['missing_values']}\")\n",
    "        \n",
    "    except NameError:\n",
    "        print(\"⚠️ Usando procesamiento básico (módulo personalizado no disponible)\")\n",
    "        \n",
    "        # Procesamiento básico manual\n",
    "        df_processed = df_raw.copy()\n",
    "        \n",
    "        # Limpiar nombres de columnas\n",
    "        df_processed.columns = df_processed.columns.str.strip().str.lower()\n",
    "        \n",
    "        # Convertir valores numéricos\n",
    "        if 'value' in df_processed.columns:\n",
    "            df_processed['value'] = pd.to_numeric(df_processed['value'], errors='coerce')\n",
    "        \n",
    "        print(f\"✅ Procesamiento básico completado\")\n",
    "        print(f\"📐 Forma final: {df_processed.shape}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd6478d",
   "metadata": {},
   "source": [
    "## 4. 🔍 Análisis Exploratorio de Datos (EDA)\n",
    "\n",
    "Exploración detallada de las variables clave de la fuerza laboral chilena, incluyendo estadísticas descriptivas, detección de patrones y análisis de outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc9d98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análisis descriptivo de variables principales\n",
    "if 'df_processed' in locals():\n",
    "    print(\"📊 ANÁLISIS DESCRIPTIVO\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Análisis de la variable principal (Value/Fuerza de Trabajo)\n",
    "    value_col = 'value' if 'value' in df_processed.columns else df_processed.select_dtypes(include=[np.number]).columns[0]\n",
    "    \n",
    "    print(f\"📈 Variable analizada: {value_col}\")\n",
    "    print(f\"📊 Estadísticas descriptivas:\")\n",
    "    \n",
    "    # Estadísticas básicas\n",
    "    stats_basic = df_processed[value_col].describe()\n",
    "    print(f\"   Media: {stats_basic['mean']:,.0f}\")\n",
    "    print(f\"   Mediana: {stats_basic['50%']:,.0f}\")\n",
    "    print(f\"   Desviación estándar: {stats_basic['std']:,.0f}\")\n",
    "    print(f\"   Mínimo: {stats_basic['min']:,.0f}\")\n",
    "    print(f\"   Máximo: {stats_basic['max']:,.0f}\")\n",
    "    \n",
    "    # Coeficiente de variación\n",
    "    cv = (stats_basic['std'] / stats_basic['mean']) * 100\n",
    "    print(f\"   Coeficiente de variación: {cv:.2f}%\")\n",
    "    \n",
    "    # Análisis por dimensiones\n",
    "    print(f\"\\n🌍 ANÁLISIS POR DIMENSIONES:\")\n",
    "    \n",
    "    # Por región (si existe)\n",
    "    region_cols = [col for col in df_processed.columns if 'region' in col.lower()]\n",
    "    if region_cols:\n",
    "        region_col = region_cols[0]\n",
    "        print(f\"\\n📍 Por {region_col}:\")\n",
    "        region_stats = df_processed.groupby(region_col)[value_col].agg(['count', 'mean', 'std']).round(0)\n",
    "        display(region_stats.head(10))\n",
    "    \n",
    "    # Por género (si existe)\n",
    "    gender_cols = [col for col in df_processed.columns if any(x in col.lower() for x in ['sexo', 'gender'])]\n",
    "    if gender_cols:\n",
    "        gender_col = gender_cols[0]\n",
    "        print(f\"\\n👥 Por {gender_col}:\")\n",
    "        gender_stats = df_processed.groupby(gender_col)[value_col].agg(['count', 'mean', 'std']).round(0)\n",
    "        display(gender_stats)\n",
    "    \n",
    "    # Por tiempo (si existe fecha)\n",
    "    date_cols = [col for col in df_processed.columns if any(x in col.lower() for x in ['date', 'year', 'fecha', 'año'])]\n",
    "    if date_cols:\n",
    "        time_col = date_cols[0]\n",
    "        print(f\"\\n📅 Tendencia temporal ({time_col}):\")\n",
    "        if 'year' in time_col.lower():\n",
    "            time_stats = df_processed.groupby(time_col)[value_col].agg(['count', 'mean']).round(0)\n",
    "            display(time_stats.tail(10))  # Últimos 10 años\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "else:\n",
    "    print(\"⚠️ Datos procesados no disponibles para análisis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34abc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detección de outliers y análisis de calidad\n",
    "if 'df_processed' in locals():\n",
    "    print(\"🎯 DETECCIÓN DE OUTLIERS Y CALIDAD DE DATOS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    value_col = 'value' if 'value' in df_processed.columns else df_processed.select_dtypes(include=[np.number]).columns[0]\n",
    "    \n",
    "    # Método IQR para detección de outliers\n",
    "    Q1 = df_processed[value_col].quantile(0.25)\n",
    "    Q3 = df_processed[value_col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers_iqr = df_processed[(df_processed[value_col] < lower_bound) | \n",
    "                               (df_processed[value_col] > upper_bound)]\n",
    "    \n",
    "    print(f\"📊 Detección de outliers (Método IQR):\")\n",
    "    print(f\"   Rango normal: {lower_bound:,.0f} - {upper_bound:,.0f}\")\n",
    "    print(f\"   Outliers encontrados: {len(outliers_iqr)} ({len(outliers_iqr)/len(df_processed)*100:.2f}%)\")\n",
    "    \n",
    "    # Método Z-Score para outliers extremos\n",
    "    z_scores = np.abs(stats.zscore(df_processed[value_col]))\n",
    "    outliers_zscore = df_processed[z_scores > 3]\n",
    "    \n",
    "    print(f\"\\n📊 Outliers extremos (Z-score > 3):\")\n",
    "    print(f\"   Outliers extremos: {len(outliers_zscore)} ({len(outliers_zscore)/len(df_processed)*100:.2f}%)\")\n",
    "    \n",
    "    if len(outliers_zscore) > 0:\n",
    "        print(f\"   Valores extremos:\")\n",
    "        extreme_values = outliers_zscore[value_col].sort_values(ascending=False)\n",
    "        for i, (idx, val) in enumerate(extreme_values.head(5).items()):\n",
    "            print(f\"      {i+1}. {val:,.0f}\")\n",
    "    \n",
    "    # Análisis de distribución\n",
    "    print(f\"\\n📈 Análisis de distribución:\")\n",
    "    \n",
    "    # Test de normalidad (Shapiro-Wilk para muestras pequeñas, Anderson-Darling para grandes)\n",
    "    if len(df_processed) <= 5000:\n",
    "        shapiro_stat, shapiro_p = stats.shapiro(df_processed[value_col].dropna().sample(min(5000, len(df_processed))))\n",
    "        print(f\"   Test de Shapiro-Wilk: estadístico={shapiro_stat:.4f}, p-valor={shapiro_p:.6f}\")\n",
    "        is_normal = shapiro_p > 0.05\n",
    "    else:\n",
    "        # Para muestras grandes, usar test de Anderson-Darling\n",
    "        anderson_result = stats.anderson(df_processed[value_col].dropna())\n",
    "        print(f\"   Test de Anderson-Darling: estadístico={anderson_result.statistic:.4f}\")\n",
    "        is_normal = anderson_result.statistic < anderson_result.critical_values[2]  # 5% level\n",
    "    \n",
    "    print(f\"   ¿Distribución normal?: {'Sí' if is_normal else 'No'}\")\n",
    "    \n",
    "    # Skewness y Kurtosis\n",
    "    skewness = stats.skew(df_processed[value_col].dropna())\n",
    "    kurtosis = stats.kurtosis(df_processed[value_col].dropna())\n",
    "    \n",
    "    print(f\"   Asimetría (skewness): {skewness:.3f}\")\n",
    "    print(f\"   Curtosis (kurtosis): {kurtosis:.3f}\")\n",
    "    \n",
    "    # Interpretación\n",
    "    if abs(skewness) < 0.5:\n",
    "        skew_interp = \"aproximadamente simétrica\"\n",
    "    elif abs(skewness) < 1:\n",
    "        skew_interp = \"moderadamente asimétrica\"\n",
    "    else:\n",
    "        skew_interp = \"altamente asimétrica\"\n",
    "    \n",
    "    print(f\"   Interpretación: Distribución {skew_interp}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "else:\n",
    "    print(\"⚠️ Datos procesados no disponibles para análisis de outliers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efe98cb",
   "metadata": {},
   "source": [
    "## 5. 📊 Visualización Profesional de la Fuerza de Trabajo\n",
    "\n",
    "Creación de gráficos ejecutivos y dashboards interactivos para comunicar los hallazgos de manera clara y profesional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff26fa79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizaciones básicas con matplotlib y seaborn\n",
    "if 'df_processed' in locals():\n",
    "    print(\"📈 CREANDO VISUALIZACIONES PROFESIONALES\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    value_col = 'value' if 'value' in df_processed.columns else df_processed.select_dtypes(include=[np.number]).columns[0]\n",
    "    \n",
    "    # Configuración de estilo\n",
    "    plt.style.use('seaborn-v0_8')\n",
    "    fig_size = (15, 10)\n",
    "    \n",
    "    # 1. Distribución de la variable principal\n",
    "    plt.figure(figsize=fig_size)\n",
    "    \n",
    "    # Subplot 1: Histograma\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.hist(df_processed[value_col] / 1000, bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    plt.title('Distribución de la Fuerza de Trabajo', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Miles de personas')\n",
    "    plt.ylabel('Frecuencia')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Subplot 2: Box plot\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.boxplot(df_processed[value_col] / 1000)\n",
    "    plt.title('Diagrama de Caja', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('Miles de personas')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Subplot 3: Q-Q Plot\n",
    "    plt.subplot(2, 2, 3)\n",
    "    stats.probplot(df_processed[value_col].dropna(), dist=\"norm\", plot=plt)\n",
    "    plt.title('Q-Q Plot (Test de Normalidad)', fontsize=14, fontweight='bold')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Subplot 4: Análisis temporal (si existe)\n",
    "    plt.subplot(2, 2, 4)\n",
    "    \n",
    "    # Buscar columna de tiempo\n",
    "    time_cols = [col for col in df_processed.columns if any(x in col.lower() for x in ['date', 'year', 'fecha', 'año'])]\\n    \n",
    "    if time_cols:\n",
    "        time_col = time_cols[0]\n",
    "        \n",
    "        # Agrupar por tiempo y calcular promedio\n",
    "        if 'date' in time_col.lower():\n",
    "            time_data = df_processed.groupby(time_col)[value_col].mean() / 1000\n",
    "        else:\n",
    "            time_data = df_processed.groupby(time_col)[value_col].mean() / 1000\n",
    "        \n",
    "        plt.plot(time_data.index, time_data.values, marker='o', linewidth=2, color='coral')\n",
    "        plt.title('Evolución Temporal', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Tiempo')\n",
    "        plt.ylabel('Miles de personas (promedio)')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, 'No hay datos temporales\\\\ndisponibles', \n",
    "                ha='center', va='center', transform=plt.gca().transAxes,\n",
    "                fontsize=12, bbox=dict(boxstyle='round', facecolor='lightgray'))\n",
    "        plt.title('Evolución Temporal', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"✅ Visualizaciones básicas completadas\")\n",
    "else:\n",
    "    print(\"⚠️ Datos no disponibles para visualización\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ffd938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizaciones por dimensiones (región, género, tiempo)\n",
    "if 'df_processed' in locals():\n",
    "    print(\"🌍 ANÁLISIS POR DIMENSIONES\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    value_col = 'value' if 'value' in df_processed.columns else df_processed.select_dtypes(include=[np.number]).columns[0]\n",
    "    \n",
    "    # Análisis por región\n",
    "    region_cols = [col for col in df_processed.columns if 'region' in col.lower()]\n",
    "    if region_cols:\n",
    "        region_col = region_cols[0]\n",
    "        \n",
    "        # Top 10 regiones por fuerza de trabajo promedio\n",
    "        plt.figure(figsize=(14, 8))\n",
    "        \n",
    "        region_avg = df_processed.groupby(region_col)[value_col].mean().sort_values(ascending=True) / 1000\n",
    "        top_regions = region_avg.tail(10)\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        bars = plt.barh(range(len(top_regions)), top_regions.values, color='lightcoral')\n",
    "        plt.yticks(range(len(top_regions)), top_regions.index)\n",
    "        plt.title('Top 10 Regiones\\\\n(Fuerza de Trabajo Promedio)', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Miles de personas')\n",
    "        plt.grid(True, alpha=0.3, axis='x')\n",
    "        \n",
    "        # Añadir valores en las barras\n",
    "        for i, bar in enumerate(bars):\n",
    "            width = bar.get_width()\n",
    "            plt.text(width + width*0.01, bar.get_y() + bar.get_height()/2, \n",
    "                    f'{width:.0f}', ha='left', va='center', fontsize=10)\n",
    "        \n",
    "        # Distribución de varianza regional\n",
    "        plt.subplot(1, 2, 2)\n",
    "        region_std = df_processed.groupby(region_col)[value_col].std() / 1000\n",
    "        plt.scatter(region_avg.values, region_std.values, alpha=0.6, s=60, color='darkblue')\n",
    "        plt.xlabel('Promedio (miles de personas)')\n",
    "        plt.ylabel('Desviación estándar (miles)')\n",
    "        plt.title('Variabilidad Regional\\\\n(Promedio vs Varianza)', fontsize=14, fontweight='bold')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Añadir línea de tendencia\n",
    "        z = np.polyfit(region_avg.values, region_std.values, 1)\n",
    "        p = np.poly1d(z)\n",
    "        plt.plot(region_avg.values, p(region_avg.values), \\\"r--\\\", alpha=0.8)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Análisis por género\n",
    "    gender_cols = [col for col in df_processed.columns if any(x in col.lower() for x in ['sexo', 'gender'])]\n",
    "    if gender_cols:\n",
    "        gender_col = gender_cols[0]\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # Comparación por género\n",
    "        plt.subplot(1, 2, 1)\n",
    "        gender_data = df_processed.groupby(gender_col)[value_col].mean() / 1000\n",
    "        colors = ['lightblue', 'lightpink', 'lightgreen'][:len(gender_data)]\n",
    "        \n",
    "        bars = plt.bar(gender_data.index, gender_data.values, color=colors, alpha=0.8, edgecolor='black')\n",
    "        plt.title('Fuerza de Trabajo por Género\\\\n(Promedio)', fontsize=14, fontweight='bold')\n",
    "        plt.ylabel('Miles de personas')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # Añadir valores en las barras\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            plt.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "                    f'{height:.0f}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "        \n",
    "        # Box plot por género\n",
    "        plt.subplot(1, 2, 2)\n",
    "        gender_groups = [df_processed[df_processed[gender_col] == gender][value_col] / 1000 \n",
    "                        for gender in df_processed[gender_col].unique()]\n",
    "        \n",
    "        plt.boxplot(gender_groups, labels=df_processed[gender_col].unique())\n",
    "        plt.title('Distribución por Género', fontsize=14, fontweight='bold')\n",
    "        plt.ylabel('Miles de personas')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    print(\"✅ Análisis dimensional completado\")\n",
    "else:\n",
    "    print(\"⚠️ Datos no disponibles para análisis dimensional\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0969b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dashboard interactivo con Plotly\n",
    "if 'df_processed' in locals():\n",
    "    print(\"🚀 DASHBOARD INTERACTIVO\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    value_col = 'value' if 'value' in df_processed.columns else df_processed.select_dtypes(include=[np.number]).columns[0]\n",
    "    \n",
    "    try:\n",
    "        # Crear dashboard usando módulo personalizado\n",
    "        chart_maker = LabourForceCharts()\n",
    "        interactive_fig = chart_maker.create_interactive_dashboard(\n",
    "            df_processed, \n",
    "            title=\"Dashboard Interactivo - Fuerza de Trabajo Chile\"\n",
    "        )\n",
    "        interactive_fig.show()\n",
    "        print(\"✅ Dashboard interactivo creado con módulo personalizado\")\n",
    "        \n",
    "    except NameError:\n",
    "        print(\"⚠️ Creando dashboard básico (módulo personalizado no disponible)\")\n",
    "        \n",
    "        # Dashboard básico con Plotly\n",
    "        from plotly.subplots import make_subplots\n",
    "        \n",
    "        fig = make_subplots(\n",
    "            rows=2, cols=2,\n",
    "            subplot_titles=('Distribución General', 'Evolución Temporal', \n",
    "                          'Por Región (Top 10)', 'Por Género'),\n",
    "            specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    "                   [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
    "        )\n",
    "        \n",
    "        # 1. Histograma de distribución\n",
    "        fig.add_trace(\n",
    "            go.Histogram(x=df_processed[value_col] / 1000, \n",
    "                        name='Distribución', \n",
    "                        nbinsx=30,\n",
    "                        marker_color='lightblue'),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        # 2. Evolución temporal (si existe)\n",
    "        time_cols = [col for col in df_processed.columns if any(x in col.lower() for x in ['date', 'year', 'fecha', 'año'])]\n",
    "        if time_cols:\n",
    "            time_col = time_cols[0]\n",
    "            time_data = df_processed.groupby(time_col)[value_col].mean() / 1000\n",
    "            \n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=time_data.index, y=time_data.values,\n",
    "                          mode='lines+markers',\n",
    "                          name='Evolución',\n",
    "                          line=dict(color='coral', width=2)),\n",
    "                row=1, col=2\n",
    "            )\n",
    "        \n",
    "        # 3. Top regiones (si existe)\n",
    "        region_cols = [col for col in df_processed.columns if 'region' in col.lower()]\n",
    "        if region_cols:\n",
    "            region_col = region_cols[0]\n",
    "            region_data = df_processed.groupby(region_col)[value_col].mean().sort_values(ascending=True).tail(10) / 1000\n",
    "            \n",
    "            fig.add_trace(\n",
    "                go.Bar(y=region_data.index, x=region_data.values,\n",
    "                      orientation='h',\n",
    "                      name='Regiones',\n",
    "                      marker_color='lightgreen'),\n",
    "                row=2, col=1\n",
    "            )\n",
    "        \n",
    "        # 4. Por género (si existe)\n",
    "        gender_cols = [col for col in df_processed.columns if any(x in col.lower() for x in ['sexo', 'gender'])]\n",
    "        if gender_cols:\n",
    "            gender_col = gender_cols[0]\n",
    "            gender_data = df_processed.groupby(gender_col)[value_col].mean() / 1000\n",
    "            \n",
    "            fig.add_trace(\n",
    "                go.Bar(x=gender_data.index, y=gender_data.values,\n",
    "                      name='Género',\n",
    "                      marker_color='lightcoral'),\n",
    "                row=2, col=2\n",
    "            )\n",
    "        \n",
    "        # Configurar layout\n",
    "        fig.update_layout(\n",
    "            title_text=\"Dashboard Fuerza de Trabajo Chile\",\n",
    "            height=800,\n",
    "            showlegend=False,\n",
    "            template='plotly_white'\n",
    "        )\n",
    "        \n",
    "        # Actualizar etiquetas de ejes\n",
    "        fig.update_xaxes(title_text=\"Miles de personas\", row=1, col=1)\n",
    "        fig.update_yaxes(title_text=\"Frecuencia\", row=1, col=1)\n",
    "        \n",
    "        if time_cols:\n",
    "            fig.update_xaxes(title_text=\"Tiempo\", row=1, col=2)\n",
    "            fig.update_yaxes(title_text=\"Miles de personas\", row=1, col=2)\n",
    "        \n",
    "        if region_cols:\n",
    "            fig.update_xaxes(title_text=\"Miles de personas\", row=2, col=1)\n",
    "            fig.update_yaxes(title_text=\"Región\", row=2, col=1)\n",
    "        \n",
    "        if gender_cols:\n",
    "            fig.update_xaxes(title_text=\"Género\", row=2, col=2)\n",
    "            fig.update_yaxes(title_text=\"Miles de personas\", row=2, col=2)\n",
    "        \n",
    "        fig.show()\n",
    "        print(\"✅ Dashboard básico completado\")\n",
    "    \n",
    "    print(\"\\n💡 El dashboard es interactivo: puedes hacer zoom, filtrar y explorar los datos\")\n",
    "    print(\"=\" * 50)\n",
    "else:\n",
    "    print(\"⚠️ Datos no disponibles para dashboard\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c9dcac",
   "metadata": {},
   "source": [
    "## 6. 🧠 Generación de Insights y Modelos Predictivos\n",
    "\n",
    "Identificación de patrones clave, generación de insights económicos y construcción de modelos predictivos cuando los datos lo permiten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e7d851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generación de insights principales\n",
    "if 'df_processed' in locals():\n",
    "    print(\"💡 INSIGHTS ECONÓMICOS PRINCIPALES\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    value_col = 'value' if 'value' in df_processed.columns else df_processed.select_dtypes(include=[np.number]).columns[0]\n",
    "    insights = []\n",
    "    \n",
    "    # 1. Insight general sobre magnitud\n",
    "    total_workforce = df_processed[value_col].sum()\n",
    "    avg_workforce = df_processed[value_col].mean()\n",
    "    \n",
    "    insights.append(f\"📊 La fuerza de trabajo total analizada alcanza {total_workforce/1000000:.1f} millones de personas\")\n",
    "    insights.append(f\"📈 El promedio por observación es de {avg_workforce/1000:.0f} mil personas\")\n",
    "    \n",
    "    # 2. Insights temporales\n",
    "    time_cols = [col for col in df_processed.columns if any(x in col.lower() for x in ['date', 'year', 'fecha', 'año'])]\n",
    "    if time_cols:\n",
    "        time_col = time_cols[0]\n",
    "        \n",
    "        # Calcular tendencia temporal\n",
    "        time_data = df_processed.groupby(time_col)[value_col].sum()\n",
    "        \n",
    "        if len(time_data) > 2:\n",
    "            # Calcular tasa de crecimiento\n",
    "            first_period = time_data.iloc[0]\n",
    "            last_period = time_data.iloc[-1]\n",
    "            periods = len(time_data) - 1\n",
    "            \n",
    "            growth_rate = ((last_period / first_period) ** (1/periods) - 1) * 100\n",
    "            total_growth = ((last_period / first_period) - 1) * 100\n",
    "            \n",
    "            insights.append(f\"📅 Tendencia temporal: crecimiento promedio anual del {growth_rate:.2f}%\")\n",
    "            insights.append(f\"📈 Crecimiento total en el período: {total_growth:.1f}%\")\n",
    "            \n",
    "            # Identificar período de mayor/menor actividad\n",
    "            max_period = time_data.idxmax()\n",
    "            min_period = time_data.idxmin()\n",
    "            \n",
    "            insights.append(f\"🔝 Período de mayor actividad: {max_period} ({time_data[max_period]/1000000:.2f}M personas)\")\n",
    "            insights.append(f\"🔻 Período de menor actividad: {min_period} ({time_data[min_period]/1000000:.2f}M personas)\")\n",
    "    \n",
    "    # 3. Insights regionales\n",
    "    region_cols = [col for col in df_processed.columns if 'region' in col.lower()]\n",
    "    if region_cols:\n",
    "        region_col = region_cols[0]\n",
    "        \n",
    "        region_totals = df_processed.groupby(region_col)[value_col].sum().sort_values(ascending=False)\n",
    "        \n",
    "        # Región líder\n",
    "        top_region = region_totals.index[0]\n",
    "        top_value = region_totals.iloc[0]\n",
    "        \n",
    "        # Concentración regional\n",
    "        top_3_share = region_totals.head(3).sum() / region_totals.sum() * 100\n",
    "        \n",
    "        insights.append(f\"🌍 Región líder: {top_region} ({top_value/1000000:.1f}M personas)\")\n",
    "        insights.append(f\"🎯 Las top 3 regiones concentran el {top_3_share:.1f}% de la fuerza laboral\")\n",
    "        \n",
    "        # Disparidad regional\n",
    "        region_cv = (region_totals.std() / region_totals.mean()) * 100\n",
    "        insights.append(f\"📊 Disparidad regional (CV): {region_cv:.1f}% ({'Alta' if region_cv > 50 else 'Moderada' if region_cv > 30 else 'Baja'})\")\n",
    "    \n",
    "    # 4. Insights de género\n",
    "    gender_cols = [col for col in df_processed.columns if any(x in col.lower() for x in ['sexo', 'gender'])]\n",
    "    if gender_cols:\n",
    "        gender_col = gender_cols[0]\n",
    "        \n",
    "        gender_totals = df_processed.groupby(gender_col)[value_col].sum()\n",
    "        \n",
    "        if 'M' in gender_totals.index and 'F' in gender_totals.index:\n",
    "            male_total = gender_totals.get('M', 0)\n",
    "            female_total = gender_totals.get('F', 0)\n",
    "            \n",
    "            if male_total > 0 and female_total > 0:\n",
    "                gender_ratio = male_total / female_total\n",
    "                female_participation = female_total / (male_total + female_total) * 100\n",
    "                \n",
    "                insights.append(f\"👥 Participación femenina: {female_participation:.1f}% del total\")\n",
    "                insights.append(f\"⚖️ Ratio hombres/mujeres: {gender_ratio:.2f}\")\n",
    "                \n",
    "                if gender_ratio > 1.5:\n",
    "                    insights.append(\"📊 Existe una brecha significativa de género en participación laboral\")\n",
    "                elif gender_ratio < 1.1:\n",
    "                    insights.append(\"📊 La participación laboral está relativamente equilibrada por género\")\n",
    "    \n",
    "    # 5. Insights de variabilidad\n",
    "    cv = (df_processed[value_col].std() / df_processed[value_col].mean()) * 100\n",
    "    insights.append(f\"📈 Variabilidad general (CV): {cv:.1f}% ({'Alta' if cv > 30 else 'Moderada' if cv > 15 else 'Baja'})\")\n",
    "    \n",
    "    # Mostrar insights\n",
    "    print(\"🔍 PRINCIPALES HALLAZGOS:\")\n",
    "    for i, insight in enumerate(insights, 1):\n",
    "        print(f\"{i:2d}. {insight}\")\n",
    "    \n",
    "    print(\"\\\\n\" + \"=\"*60)\n",
    "    \n",
    "    # Generar recomendaciones\n",
    "    print(\"\\\\n🎯 RECOMENDACIONES ESTRATÉGICAS:\")\n",
    "    recommendations = []\n",
    "    \n",
    "    if 'growth_rate' in locals() and growth_rate > 2:\n",
    "        recommendations.append(\"📈 El crecimiento sostenido sugiere políticas de empleo efectivas\")\n",
    "    elif 'growth_rate' in locals() and growth_rate < 0:\n",
    "        recommendations.append(\"⚠️ El decrecimiento requiere políticas de estímulo al empleo\")\n",
    "    \n",
    "    if 'region_cv' in locals() and region_cv > 50:\n",
    "        recommendations.append(\"🌍 Alta disparidad regional requiere políticas de desarrollo territorial\")\n",
    "    \n",
    "    if 'female_participation' in locals() and female_participation < 40:\n",
    "        recommendations.append(\"👩 Baja participación femenina sugiere necesidad de políticas de inclusión\")\n",
    "    \n",
    "    if cv > 30:\n",
    "        recommendations.append(\"📊 Alta variabilidad indica mercado laboral heterogéneo\")\n",
    "    \n",
    "    if not recommendations:\n",
    "        recommendations.append(\"📊 Los indicadores muestran un mercado laboral relativamente estable\")\n",
    "    \n",
    "    for i, rec in enumerate(recommendations, 1):\n",
    "        print(f\"{i}. {rec}\")\n",
    "    \n",
    "    print(\"\\\\n\" + \"=\"*60)\n",
    "else:\n",
    "    print(\"⚠️ Datos no disponibles para generación de insights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80705381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo predictivo básico (si hay datos temporales)\n",
    "if 'df_processed' in locals():\n",
    "    print(\"🤖 MODELO PREDICTIVO BÁSICO\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    time_cols = [col for col in df_processed.columns if any(x in col.lower() for x in ['date', 'year', 'fecha', 'año'])]\n",
    "    value_col = 'value' if 'value' in df_processed.columns else df_processed.select_dtypes(include=[np.number]).columns[0]\n",
    "    \n",
    "    if time_cols:\n",
    "        time_col = time_cols[0]\n",
    "        \n",
    "        # Preparar datos para modelado\n",
    "        # Filtrar datos nacionales para el modelo\n",
    "        national_filter = df_processed['region_code'] == '_T' if 'region_code' in df_processed.columns else True\n",
    "        gender_filter = df_processed['gender_code'] == '_T' if 'gender_code' in df_processed.columns else True\n",
    "        \n",
    "        if isinstance(national_filter, bool):\n",
    "            model_data = df_processed[gender_filter] if not isinstance(gender_filter, bool) else df_processed\n",
    "        else:\n",
    "            model_data = df_processed[national_filter & gender_filter] if not isinstance(gender_filter, bool) else df_processed[national_filter]\n",
    "        \n",
    "        if len(model_data) > 5:  # Necesitamos suficientes datos\n",
    "            # Crear series temporal\n",
    "            ts_data = model_data.groupby(time_col)[value_col].mean().sort_index()\n",
    "            \n",
    "            print(f\"📊 Datos para modelado: {len(ts_data)} observaciones temporales\")\n",
    "            \n",
    "            # Modelo de tendencia lineal simple\n",
    "            from sklearn.linear_model import LinearRegression\n",
    "            from sklearn.metrics import mean_absolute_error, r2_score\n",
    "            \n",
    "            # Preparar variables\n",
    "            X = np.arange(len(ts_data)).reshape(-1, 1)  # Tiempo como variable numérica\n",
    "            y = ts_data.values\n",
    "            \n",
    "            # Dividir en entrenamiento y prueba (80-20)\n",
    "            split_idx = int(len(X) * 0.8)\n",
    "            X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "            y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "            \n",
    "            # Entrenar modelo\n",
    "            model = LinearRegression()\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            # Predicciones\n",
    "            y_pred_train = model.predict(X_train)\n",
    "            y_pred_test = model.predict(X_test) if len(X_test) > 0 else []\n",
    "            \n",
    "            # Métricas\n",
    "            train_r2 = r2_score(y_train, y_pred_train)\n",
    "            train_mae = mean_absolute_error(y_train, y_pred_train)\n",
    "            \n",
    "            print(f\"✅ Modelo entrenado exitosamente\")\n",
    "            print(f\"📈 R² (entrenamiento): {train_r2:.3f}\")\n",
    "            print(f\"📊 MAE (entrenamiento): {train_mae/1000:.0f} miles de personas\")\n",
    "            \n",
    "            if len(y_test) > 0:\n",
    "                test_r2 = r2_score(y_test, y_pred_test)\n",
    "                test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "                print(f\"🎯 R² (prueba): {test_r2:.3f}\")\n",
    "                print(f\"📊 MAE (prueba): {test_mae/1000:.0f} miles de personas\")\n",
    "            \n",
    "            # Proyección futura (próximos 4 períodos)\n",
    "            future_periods = 4\n",
    "            X_future = np.arange(len(ts_data), len(ts_data) + future_periods).reshape(-1, 1)\n",
    "            y_future = model.predict(X_future)\n",
    "            \n",
    "            print(f\"\\\\n🔮 PROYECCIONES FUTURAS ({future_periods} períodos):\")\n",
    "            for i, pred in enumerate(y_future, 1):\n",
    "                print(f\"   Período +{i}: {pred/1000:.0f} miles de personas\")\n",
    "            \n",
    "            # Visualización del modelo\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            \n",
    "            # Datos históricos\n",
    "            plt.plot(range(len(ts_data)), ts_data.values/1000, 'o-', label='Datos reales', color='blue', alpha=0.7)\n",
    "            \n",
    "            # Predicciones del modelo\n",
    "            plt.plot(range(len(y_train)), y_pred_train/1000, '--', label='Predicción (entrenamiento)', color='red', alpha=0.8)\n",
    "            \n",
    "            if len(y_test) > 0:\n",
    "                plt.plot(range(len(y_train), len(ts_data)), y_pred_test/1000, '--', label='Predicción (prueba)', color='orange', alpha=0.8)\n",
    "            \n",
    "            # Proyecciones futuras\n",
    "            future_x = range(len(ts_data), len(ts_data) + future_periods)\n",
    "            plt.plot(future_x, y_future/1000, 's--', label='Proyección futura', color='green', alpha=0.8)\n",
    "            \n",
    "            # Configuración del gráfico\n",
    "            plt.title('Modelo Predictivo de Fuerza de Trabajo', fontsize=14, fontweight='bold')\n",
    "            plt.xlabel('Período')\n",
    "            plt.ylabel('Miles de personas')\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Añadir línea divisoria entre datos y proyección\n",
    "            plt.axvline(x=len(ts_data)-0.5, color='gray', linestyle=':', alpha=0.7, label='Inicio proyección')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Interpretación del modelo\n",
    "            slope = model.coef_[0]\n",
    "            trend_interpretation = \"creciente\" if slope > 0 else \"decreciente\" if slope < 0 else \"estable\"\n",
    "            \n",
    "            print(f\"\\\\n📊 INTERPRETACIÓN DEL MODELO:\")\n",
    "            print(f\"   Tendencia: {trend_interpretation}\")\n",
    "            print(f\"   Cambio promedio por período: {slope/1000:+.0f} miles de personas\")\n",
    "            \n",
    "            if train_r2 > 0.8:\n",
    "                print(f\"   Calidad del modelo: Excelente (R² = {train_r2:.3f})\")\n",
    "            elif train_r2 > 0.6:\n",
    "                print(f\"   Calidad del modelo: Buena (R² = {train_r2:.3f})\")\n",
    "            elif train_r2 > 0.4:\n",
    "                print(f\"   Calidad del modelo: Moderada (R² = {train_r2:.3f})\")\n",
    "            else:\n",
    "                print(f\"   Calidad del modelo: Baja (R² = {train_r2:.3f}) - datos muy variables\")\n",
    "            \n",
    "        else:\n",
    "            print(\"⚠️ Insuficientes datos temporales para modelado\")\n",
    "    else:\n",
    "        print(\"⚠️ No se encontraron columnas temporales para modelado predictivo\")\n",
    "        print(\"💡 Sugerencia: Incluir variables temporales para habilitar predicciones\")\n",
    "    \n",
    "    print(\"\\\\n\" + \"=\"*50)\n",
    "else:\n",
    "    print(\"⚠️ Datos no disponibles para modelado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d8a695",
   "metadata": {},
   "source": [
    "## 7. 🏗️ Buenas Prácticas y Estructura Modular del Proyecto\n",
    "\n",
    "Demostración de la organización profesional del código, aplicación de principios SOLID y Clean Code, y documentación de la arquitectura del proyecto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5539019e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Documentación de la estructura del proyecto\n",
    "print(\"🏗️ ESTRUCTURA MODULAR DEL PROYECTO\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\"\"\n",
    "📁 ine-chile-labour-force-analysis/\n",
    "├── 📊 data/\n",
    "│   ├── raw/           # Datos crudos del INE (CSV originales)\n",
    "│   ├── processed/     # Datos limpios y transformados\n",
    "│   └── outputs/       # Resultados finales y exportaciones\n",
    "├── 📝 src/           # Código fuente modular\n",
    "│   ├── etl/          # Extract, Transform, Load\n",
    "│   │   ├── __init__.py\n",
    "│   │   ├── base.py           # Clases base abstractas\n",
    "│   │   └── data_processor.py # Procesador de datos del INE\n",
    "│   ├── models/       # Modelos estadísticos y ML\n",
    "│   │   └── __init__.py\n",
    "│   ├── utils/        # Utilidades y helpers\n",
    "│   │   ├── __init__.py\n",
    "│   │   ├── logger_config.py  # Configuración de logging\n",
    "│   │   └── validators.py     # Validadores de datos\n",
    "│   └── visualization/ # Gráficos y dashboards\n",
    "│       ├── __init__.py\n",
    "│       └── charts.py         # Clases para visualización\n",
    "├── 📔 notebooks/     # Jupyter notebooks de análisis\n",
    "│   └── 01_eda_labour_force.ipynb\n",
    "├── 🔧 scripts/       # Scripts de automatización\n",
    "├── 🧪 tests/         # Tests unitarios e integración\n",
    "├── 📚 docs/          # Documentación del proyecto\n",
    "├── 📋 config.py      # Configuración centralizada\n",
    "├── 📄 requirements.txt # Dependencias\n",
    "├── ⚙️ setup.py       # Configuración del paquete\n",
    "└── 📖 README.md      # Documentación principal\n",
    "\"\"\")\n",
    "\n",
    "print(\"🎯 PRINCIPIOS APLICADOS:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "principles = [\n",
    "    (\"Clean Code\", [\n",
    "        \"Nombres descriptivos y funciones pequeñas\",\n",
    "        \"Principio DRY (Don't Repeat Yourself)\", \n",
    "        \"Comentarios claros y documentación exhaustiva\",\n",
    "        \"Código auto-documentado y legible\"\n",
    "    ]),\n",
    "    \n",
    "    (\"SOLID Principles\", [\n",
    "        \"S - Single Responsibility: Cada clase tiene una responsabilidad\",\n",
    "        \"O - Open/Closed: Abierto para extensión, cerrado para modificación\",\n",
    "        \"L - Liskov Substitution: Clases derivadas son sustituibles\",\n",
    "        \"I - Interface Segregation: Interfaces específicas y cohesivas\",\n",
    "        \"D - Dependency Inversion: Depender de abstracciones, no concreciones\"\n",
    "    ]),\n",
    "    \n",
    "    (\"Data Science Best Practices\", [\n",
    "        \"Validación exhaustiva de datos\",\n",
    "        \"Logging y monitoreo de procesos\",\n",
    "        \"Reproducibilidad de experimentos\",\n",
    "        \"Versionado de datos y código\",\n",
    "        \"Documentación de metodología\"\n",
    "    ])\n",
    "]\n",
    "\n",
    "for principle, practices in principles:\n",
    "    print(f\"\\\\n📚 {principle}:\")\n",
    "    for practice in practices:\n",
    "        print(f\"   ✅ {practice}\")\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f443c2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de uso de módulos profesionales y guardar resultados\n",
    "print(\"💾 GUARDADO DE RESULTADOS Y EJEMPLO DE MODULARIEDAD\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Guardar datos procesados\n",
    "if 'df_processed' in locals():\n",
    "    \n",
    "    # Crear directorio de salida si no existe\n",
    "    output_path = processed_data_path / \"labour_force_processed.csv\"\n",
    "    \n",
    "    try:\n",
    "        # Guardar datos procesados\n",
    "        df_processed.to_csv(output_path, index=False, encoding='utf-8')\n",
    "        print(f\"✅ Datos procesados guardados en: {output_path}\")\n",
    "        print(f\"📊 Registros guardados: {len(df_processed):,}\")\n",
    "        \n",
    "        # Guardar resumen estadístico\n",
    "        summary_path = output_data_path / \"summary_statistics.txt\"\n",
    "        \n",
    "        with open(summary_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(\"RESUMEN ESTADÍSTICO - FUERZA DE TRABAJO CHILE\\\\n\")\n",
    "            f.write(\"=\" * 50 + \"\\\\n\\\\n\")\n",
    "            \n",
    "            f.write(f\"Fecha de análisis: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\\\\n\")\n",
    "            f.write(f\"Total de registros: {len(df_processed):,}\\\\n\")\n",
    "            f.write(f\"Período analizado: {df_processed['date'].min()} a {df_processed['date'].max()}\\\\n\" if 'date' in df_processed.columns else \"\")\n",
    "            f.write(f\"Regiones incluidas: {df_processed['region_code'].nunique()}\\\\n\" if 'region_code' in df_processed.columns else \"\")\n",
    "            \n",
    "            # Estadísticas principales\n",
    "            value_col = 'value' if 'value' in df_processed.columns else df_processed.select_dtypes(include=[np.number]).columns[0]\n",
    "            stats = df_processed[value_col].describe()\n",
    "            \n",
    "            f.write(\"\\\\nESTADÍSTICAS PRINCIPALES:\\\\n\")\n",
    "            f.write(f\"Promedio: {stats['mean']:,.0f}\\\\n\")\n",
    "            f.write(f\"Mediana: {stats['50%']:,.0f}\\\\n\")\n",
    "            f.write(f\"Mínimo: {stats['min']:,.0f}\\\\n\")\n",
    "            f.write(f\"Máximo: {stats['max']:,.0f}\\\\n\")\n",
    "            f.write(f\"Desviación estándar: {stats['std']:,.0f}\\\\n\")\n",
    "        \n",
    "        print(f\"✅ Resumen estadístico guardado en: {summary_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error al guardar archivos: {str(e)}\")\n",
    "\n",
    "# Ejemplo de uso modular profesional\n",
    "print(\"\\\\n🔧 EJEMPLO DE USO MODULAR:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "code_example = '''\n",
    "# Ejemplo de uso profesional de los módulos\n",
    "\n",
    "# 1. Importar módulos del proyecto\n",
    "from etl.data_processor import LabourForceProcessor\n",
    "from visualization.charts import LabourForceCharts\n",
    "from utils.validators import LabourForceValidator\n",
    "\n",
    "# 2. Crear pipeline de procesamiento\n",
    "processor = LabourForceProcessor()\n",
    "\n",
    "# 3. Procesar datos con validación\n",
    "raw_data = processor.extractor.extract(\"data/raw/labour_force.csv\")\n",
    "clean_data = processor.transformer.transform(raw_data)\n",
    "processor.loader.load(clean_data, \"data/processed/labour_force_clean.csv\")\n",
    "\n",
    "# 4. Crear visualizaciones profesionales\n",
    "charts = LabourForceCharts()\n",
    "fig = charts.plot_time_series(clean_data, save_path=\"outputs/time_series.png\")\n",
    "dashboard = charts.create_interactive_dashboard(clean_data)\n",
    "\n",
    "# 5. Validar calidad de datos\n",
    "validator = LabourForceValidator()\n",
    "validation_results = validator.validate_labour_force_data(clean_data)\n",
    "report = validator.get_validation_report()\n",
    "'''\n",
    "\n",
    "print(code_example)\n",
    "\n",
    "print(\"🎯 VENTAJAS DE LA ARQUITECTURA MODULAR:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "advantages = [\n",
    "    \"🔄 Reutilización: Los módulos pueden ser reutilizados en otros proyectos\",\n",
    "    \"🧪 Testabilidad: Cada módulo puede ser testeado independientemente\", \n",
    "    \"🔧 Mantenibilidad: Cambios localizados no afectan todo el sistema\",\n",
    "    \"📈 Escalabilidad: Fácil añadir nuevas funcionalidades\",\n",
    "    \"👥 Colaboración: Múltiples desarrolladores pueden trabajar en paralelo\",\n",
    "    \"📚 Documentación: Cada módulo está auto-documentado\",\n",
    "    \"🎨 Flexibilidad: Fácil intercambiar implementaciones\",\n",
    "    \"🚀 Productividad: Menos código duplicado, más eficiencia\"\n",
    "]\n",
    "\n",
    "for advantage in advantages:\n",
    "    print(f\"   {advantage}\")\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*60)\n",
    "\n",
    "# Información del proyecto para stakeholders\n",
    "print(\"\\\\n📋 INFORMACIÓN PARA STAKEHOLDERS:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "stakeholder_info = [\n",
    "    (\"🎯 Objetivo del Proyecto\", \"Análisis avanzado de la Fuerza de Trabajo en Chile usando datos oficiales del INE\"),\n",
    "    (\"📊 Metodología\", \"Data Science con Clean Code y principios SOLID\"),\n",
    "    (\"🔍 Análisis Incluidos\", \"Exploratorio, temporal, regional, por género, predictivo\"),\n",
    "    (\"📈 Visualizaciones\", \"Gráficos ejecutivos, dashboards interactivos, mapas\"),\n",
    "    (\"🤖 Modelos\", \"Predictivos para proyecciones futuras\"),\n",
    "    (\"📚 Documentación\", \"Completa y profesional para reproducibilidad\"),\n",
    "    (\"🔧 Arquitectura\", \"Modular y escalable para mantenimiento\"),\n",
    "    (\"📄 Entregables\", \"Código, datos procesados, visualizaciones, insights\")\n",
    "]\n",
    "\n",
    "for title, description in stakeholder_info:\n",
    "    print(f\"{title}: {description}\")\n",
    "\n",
    "print(f\"\\\\n🏆 RESULTADO: Repositorio profesional evidenciando capacidades senior en Data Science\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a60eefd",
   "metadata": {},
   "source": [
    "## 🎯 Conclusiones y Próximos Pasos\n",
    "\n",
    "### 📊 Resumen Ejecutivo\n",
    "\n",
    "Este análisis exploratorio de la Fuerza de Trabajo en Chile demuestra la aplicación de metodologías avanzadas de Data Science siguiendo las mejores prácticas de la industria:\n",
    "\n",
    "### ✅ Logros Principales\n",
    "\n",
    "1. **Análisis Completo**: Exploración exhaustiva de datos del INE con validación de calidad\n",
    "2. **Arquitectura Profesional**: Implementación de Clean Code y principios SOLID\n",
    "3. **Visualizaciones Ejecutivas**: Gráficos profesionales y dashboards interactivos\n",
    "4. **Insights Económicos**: Identificación de patrones y tendencias clave\n",
    "5. **Modelos Predictivos**: Proyecciones basadas en datos históricos\n",
    "6. **Documentación Completa**: Código reproducible y bien documentado\n",
    "\n",
    "### 🚀 Próximos Pasos\n",
    "\n",
    "1. **Expansión de Datos**: Integrar más fuentes del INE (ocupación, desocupación, etc.)\n",
    "2. **Dashboard Web**: Crear aplicación web interactiva con Streamlit/Dash\n",
    "3. **Machine Learning Avanzado**: Implementar modelos de forecasting más sofisticados\n",
    "4. **API de Datos**: Desarrollar API REST para consultas automáticas\n",
    "5. **Automatización**: Crear pipelines automáticos de actualización de datos\n",
    "6. **Análisis Sectorial**: Incluir análisis por sectores económicos\n",
    "7. **Geolocalización**: Añadir mapas interactivos regionales\n",
    "\n",
    "### 🏆 Valor del Proyecto\n",
    "\n",
    "Este repositorio evidencia capacidades **senior en Data Science** a través de:\n",
    "- Código limpio y arquitectura escalable\n",
    "- Metodología científica rigurosa\n",
    "- Visualizaciones de calidad ejecutiva\n",
    "- Documentación profesional completa\n",
    "- Aplicación de mejores prácticas de la industria\n",
    "\n",
    "---\n",
    "\n",
    "**📧 Contacto**: Bruno San Martin | **🔗 GitHub**: @SanMaBruno | **📅 Fecha**: Julio 2025"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
