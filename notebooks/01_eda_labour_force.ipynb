{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03b1bd62",
   "metadata": {},
   "source": [
    "# ğŸ‡¨ğŸ‡± AnÃ¡lisis Exploratorio de la Fuerza de Trabajo en Chile\n",
    "\n",
    "## AnÃ¡lisis Avanzado de Datos del INE\n",
    "\n",
    "**Autor:** Bruno San Martin  \n",
    "**Fecha:** Julio 2025  \n",
    "**Fuente:** Instituto Nacional de EstadÃ­sticas (INE) - Chile  \n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Š Objetivos del AnÃ¡lisis\n",
    "\n",
    "1. **ExploraciÃ³n de Datos**: Entender la estructura y calidad de los datos de fuerza laboral\n",
    "2. **AnÃ¡lisis Temporal**: Identificar tendencias y patrones a lo largo del tiempo\n",
    "3. **AnÃ¡lisis Regional**: Comparar indicadores entre diferentes regiones de Chile\n",
    "4. **AnÃ¡lisis por GÃ©nero**: Examinar diferencias en participaciÃ³n laboral por sexo\n",
    "5. **Insights EconÃ³micos**: Generar conclusiones actionables para stakeholders\n",
    "\n",
    "### ğŸ¯ MetodologÃ­a\n",
    "\n",
    "- **Clean Code**: CÃ³digo limpio y bien documentado\n",
    "- **SOLID Principles**: Arquitectura modular y escalable  \n",
    "- **Data Science Best Practices**: ValidaciÃ³n, logging y reproducibilidad\n",
    "- **Professional Visualizations**: GrÃ¡ficos de calidad ejecutiva"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bf6a3d",
   "metadata": {},
   "source": [
    "## 1. ğŸ› ï¸ ConfiguraciÃ³n del Entorno y LibrerÃ­as\n",
    "\n",
    "ConfiguraciÃ³n inicial del entorno de trabajo, importaciÃ³n de librerÃ­as necesarias y configuraciÃ³n de rutas del proyecto siguiendo arquitectura modular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78971f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConfiguraciÃ³n del entorno\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# AÃ±adir el directorio src al path para importar nuestros mÃ³dulos\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.append(str(project_root / \"src\"))\n",
    "\n",
    "# Suprimir warnings innecesarios\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"ğŸ“ Directorio del proyecto: {project_root}\")\n",
    "print(f\"ğŸ Python path configurado correctamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283ec6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LibrerÃ­as principales para Data Science\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# LibrerÃ­as para anÃ¡lisis estadÃ­stico\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# LibrerÃ­as del proyecto (arquitectura modular)\n",
    "try:\n",
    "    from etl.data_processor import LabourForceProcessor\n",
    "    from visualization.charts import LabourForceCharts, StatisticalCharts\n",
    "    from utils.validators import LabourForceValidator\n",
    "    from utils.logger_config import get_logger\n",
    "    print(\"âœ… MÃ³dulos del proyecto importados correctamente\")\n",
    "except ImportError as e:\n",
    "    print(f\"âš ï¸  Error importando mÃ³dulos del proyecto: {e}\")\n",
    "    print(\"ğŸ“‹ Continuaremos con librerÃ­as estÃ¡ndar\")\n",
    "\n",
    "# ConfiguraciÃ³n de visualizaciones\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "# ConfiguraciÃ³n de plotly\n",
    "import plotly.io as pio\n",
    "pio.templates.default = \"plotly_white\"\n",
    "\n",
    "print(\"ğŸ“Š ConfiguraciÃ³n de visualizaciÃ³n completada\")\n",
    "print(\"ğŸ¯ Entorno listo para anÃ¡lisis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965aaa24",
   "metadata": {},
   "source": [
    "## 2. ğŸ“¥ ExtracciÃ³n y OrganizaciÃ³n de Datos del INE\n",
    "\n",
    "Carga y exploraciÃ³n inicial de los datos oficiales de la Fuerza de Trabajo proporcionados por el Instituto Nacional de EstadÃ­sticas (INE) de Chile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2dd0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir rutas de datos\n",
    "raw_data_path = project_root / \"data\" / \"raw\"\n",
    "processed_data_path = project_root / \"data\" / \"processed\"\n",
    "output_data_path = project_root / \"data\" / \"outputs\"\n",
    "\n",
    "# Crear directorios si no existen\n",
    "for path in [raw_data_path, processed_data_path, output_data_path]:\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"ğŸ“‚ Directorio de datos crudos: {raw_data_path}\")\n",
    "print(f\"ğŸ“‚ Directorio de datos procesados: {processed_data_path}\")\n",
    "print(f\"ğŸ“‚ Directorio de salida: {output_data_path}\")\n",
    "\n",
    "# Listar archivos disponibles en data/raw\n",
    "raw_files = list(raw_data_path.glob(\"*.csv\"))\n",
    "print(f\"\\nğŸ“‹ Archivos CSV disponibles: {len(raw_files)}\")\n",
    "for file in raw_files:\n",
    "    file_size = file.stat().st_size / (1024*1024)  # MB\n",
    "    print(f\"   ğŸ“„ {file.name} ({file_size:.2f} MB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16fc529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar datos del INE\n",
    "if raw_files:\n",
    "    # Tomar el primer archivo CSV encontrado\n",
    "    data_file = raw_files[0]\n",
    "    print(f\"ğŸ“Š Cargando datos desde: {data_file.name}\")\n",
    "    \n",
    "    # Cargar datos\n",
    "    try:\n",
    "        df_raw = pd.read_csv(data_file, encoding='utf-8')\n",
    "        print(f\"âœ… Datos cargados exitosamente\")\n",
    "        print(f\"ğŸ“ Dimensiones: {df_raw.shape}\")\n",
    "        \n",
    "    except UnicodeDecodeError:\n",
    "        # Intentar con encoding alternativo\n",
    "        df_raw = pd.read_csv(data_file, encoding='latin-1')\n",
    "        print(f\"âœ… Datos cargados con encoding latin-1\")\n",
    "        print(f\"ğŸ“ Dimensiones: {df_raw.shape}\")\n",
    "        \n",
    "else:\n",
    "    print(\"âŒ No se encontraron archivos CSV en data/raw\")\n",
    "    print(\"ğŸ“‹ Por favor, coloca el archivo de datos del INE en la carpeta data/raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1566876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ExploraciÃ³n inicial de los datos\n",
    "if 'df_raw' in locals():\n",
    "    print(\"ğŸ” EXPLORACIÃ“N INICIAL DE DATOS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # InformaciÃ³n bÃ¡sica\n",
    "    print(f\"ğŸ“Š Forma del dataset: {df_raw.shape}\")\n",
    "    print(f\"ğŸ“‹ Columnas: {list(df_raw.columns)}\")\n",
    "    \n",
    "    # Mostrar primeras filas\n",
    "    print(\"\\nğŸ“– Primeras 5 filas:\")\n",
    "    display(df_raw.head())\n",
    "    \n",
    "    # InformaciÃ³n de tipos de datos\n",
    "    print(\"\\nğŸ”¢ InformaciÃ³n de tipos de datos:\")\n",
    "    display(df_raw.info())\n",
    "    \n",
    "    # EstadÃ­sticas descriptivas\n",
    "    print(\"\\nğŸ“ˆ EstadÃ­sticas descriptivas:\")\n",
    "    display(df_raw.describe(include='all'))\n",
    "    \n",
    "    # Valores Ãºnicos en columnas categÃ³ricas\n",
    "    print(\"\\nğŸ·ï¸ Valores Ãºnicos en columnas principales:\")\n",
    "    categorical_cols = df_raw.select_dtypes(include=['object']).columns\n",
    "    for col in categorical_cols[:5]:  # Mostrar solo las primeras 5\n",
    "        unique_count = df_raw[col].nunique()\n",
    "        print(f\"   {col}: {unique_count} valores Ãºnicos\")\n",
    "        if unique_count <= 10:\n",
    "            print(f\"      Valores: {list(df_raw[col].unique())}\")\n",
    "        else:\n",
    "            print(f\"      Algunos valores: {list(df_raw[col].unique()[:5])}...\")\n",
    "else:\n",
    "    print(\"âš ï¸ Datos no disponibles para exploraciÃ³n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee6783d",
   "metadata": {},
   "source": [
    "## 3. ğŸ§¹ Procesamiento y Limpieza de Datos (ETL)\n",
    "\n",
    "AplicaciÃ³n de tÃ©cnicas de ETL (Extract, Transform, Load) para limpiar, estandarizar y transformar los datos crudos del INE en un formato Ã³ptimo para anÃ¡lisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152c7056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ValidaciÃ³n de datos usando nuestro mÃ³dulo personalizado\n",
    "if 'df_raw' in locals():\n",
    "    print(\"ğŸ” VALIDACIÃ“N DE CALIDAD DE DATOS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Crear validador\n",
    "    try:\n",
    "        validator = LabourForceValidator()\n",
    "        \n",
    "        # Ejecutar validaciones\n",
    "        validation_results = validator.validate_labour_force_data(df_raw)\n",
    "        \n",
    "        # Mostrar reporte\n",
    "        print(\"\\nğŸ“‹ REPORTE DE VALIDACIÃ“N:\")\n",
    "        report = validator.get_validation_report()\n",
    "        print(report)\n",
    "        \n",
    "    except NameError:\n",
    "        print(\"âš ï¸ Usando validaciÃ³n bÃ¡sica (mÃ³dulo personalizado no disponible)\")\n",
    "        \n",
    "        # ValidaciÃ³n bÃ¡sica manual\n",
    "        print(f\"âœ… Datos no vacÃ­os: {not df_raw.empty}\")\n",
    "        print(f\"âœ… Sin filas completamente vacÃ­as: {df_raw.dropna(how='all').shape[0] == df_raw.shape[0]}\")\n",
    "        print(f\"ğŸ“Š Valores faltantes por columna:\")\n",
    "        missing_data = df_raw.isnull().sum()\n",
    "        missing_data = missing_data[missing_data > 0]\n",
    "        if len(missing_data) == 0:\n",
    "            print(\"   ğŸ‰ No hay valores faltantes\")\n",
    "        else:\n",
    "            for col, count in missing_data.items():\n",
    "                percentage = (count / len(df_raw)) * 100\n",
    "                print(f\"   {col}: {count} ({percentage:.2f}%)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9fafc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procesamiento de datos usando arquitectura modular\n",
    "if 'df_raw' in locals():\n",
    "    print(\"ğŸ”„ PROCESAMIENTO DE DATOS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    try:\n",
    "        # Usar procesador personalizado\n",
    "        processor = LabourForceProcessor()\n",
    "        \n",
    "        # Procesar datos\n",
    "        print(\"ğŸ“Š Aplicando transformaciones...\")\n",
    "        df_processed = processor.transformer.transform(df_raw)\n",
    "        \n",
    "        print(f\"âœ… Procesamiento completado\")\n",
    "        print(f\"ğŸ“ Datos originales: {df_raw.shape}\")\n",
    "        print(f\"ğŸ“ Datos procesados: {df_processed.shape}\")\n",
    "        \n",
    "        # Generar estadÃ­sticas\n",
    "        stats = processor.get_summary_statistics(df_processed)\n",
    "        print(f\"\\nğŸ“ˆ ESTADÃSTICAS DEL PROCESAMIENTO:\")\n",
    "        print(f\"   ğŸ“Š Total de registros: {stats['total_records']:,}\")\n",
    "        print(f\"   ğŸ“… Rango de fechas: {stats['date_range']['min']} a {stats['date_range']['max']}\")\n",
    "        print(f\"   ğŸŒ Regiones: {stats['regions_count']}\")\n",
    "        print(f\"   ğŸ“† AÃ±os: {stats['years_count']}\")\n",
    "        print(f\"   â“ Valores faltantes: {stats['missing_values']}\")\n",
    "        \n",
    "    except NameError:\n",
    "        print(\"âš ï¸ Usando procesamiento bÃ¡sico (mÃ³dulo personalizado no disponible)\")\n",
    "        \n",
    "        # Procesamiento bÃ¡sico manual\n",
    "        df_processed = df_raw.copy()\n",
    "        \n",
    "        # Limpiar nombres de columnas\n",
    "        df_processed.columns = df_processed.columns.str.strip().str.lower()\n",
    "        \n",
    "        # Convertir valores numÃ©ricos\n",
    "        if 'value' in df_processed.columns:\n",
    "            df_processed['value'] = pd.to_numeric(df_processed['value'], errors='coerce')\n",
    "        \n",
    "        print(f\"âœ… Procesamiento bÃ¡sico completado\")\n",
    "        print(f\"ğŸ“ Forma final: {df_processed.shape}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd6478d",
   "metadata": {},
   "source": [
    "## 4. ğŸ” AnÃ¡lisis Exploratorio de Datos (EDA)\n",
    "\n",
    "ExploraciÃ³n detallada de las variables clave de la fuerza laboral chilena, incluyendo estadÃ­sticas descriptivas, detecciÃ³n de patrones y anÃ¡lisis de outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc9d98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AnÃ¡lisis descriptivo de variables principales\n",
    "if 'df_processed' in locals():\n",
    "    print(\"ğŸ“Š ANÃLISIS DESCRIPTIVO\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # AnÃ¡lisis de la variable principal (Value/Fuerza de Trabajo)\n",
    "    value_col = 'value' if 'value' in df_processed.columns else df_processed.select_dtypes(include=[np.number]).columns[0]\n",
    "    \n",
    "    print(f\"ğŸ“ˆ Variable analizada: {value_col}\")\n",
    "    print(f\"ğŸ“Š EstadÃ­sticas descriptivas:\")\n",
    "    \n",
    "    # EstadÃ­sticas bÃ¡sicas\n",
    "    stats_basic = df_processed[value_col].describe()\n",
    "    print(f\"   Media: {stats_basic['mean']:,.0f}\")\n",
    "    print(f\"   Mediana: {stats_basic['50%']:,.0f}\")\n",
    "    print(f\"   DesviaciÃ³n estÃ¡ndar: {stats_basic['std']:,.0f}\")\n",
    "    print(f\"   MÃ­nimo: {stats_basic['min']:,.0f}\")\n",
    "    print(f\"   MÃ¡ximo: {stats_basic['max']:,.0f}\")\n",
    "    \n",
    "    # Coeficiente de variaciÃ³n\n",
    "    cv = (stats_basic['std'] / stats_basic['mean']) * 100\n",
    "    print(f\"   Coeficiente de variaciÃ³n: {cv:.2f}%\")\n",
    "    \n",
    "    # AnÃ¡lisis por dimensiones\n",
    "    print(f\"\\nğŸŒ ANÃLISIS POR DIMENSIONES:\")\n",
    "    \n",
    "    # Por regiÃ³n (si existe)\n",
    "    region_cols = [col for col in df_processed.columns if 'region' in col.lower()]\n",
    "    if region_cols:\n",
    "        region_col = region_cols[0]\n",
    "        print(f\"\\nğŸ“ Por {region_col}:\")\n",
    "        region_stats = df_processed.groupby(region_col)[value_col].agg(['count', 'mean', 'std']).round(0)\n",
    "        display(region_stats.head(10))\n",
    "    \n",
    "    # Por gÃ©nero (si existe)\n",
    "    gender_cols = [col for col in df_processed.columns if any(x in col.lower() for x in ['sexo', 'gender'])]\n",
    "    if gender_cols:\n",
    "        gender_col = gender_cols[0]\n",
    "        print(f\"\\nğŸ‘¥ Por {gender_col}:\")\n",
    "        gender_stats = df_processed.groupby(gender_col)[value_col].agg(['count', 'mean', 'std']).round(0)\n",
    "        display(gender_stats)\n",
    "    \n",
    "    # Por tiempo (si existe fecha)\n",
    "    date_cols = [col for col in df_processed.columns if any(x in col.lower() for x in ['date', 'year', 'fecha', 'aÃ±o'])]\n",
    "    if date_cols:\n",
    "        time_col = date_cols[0]\n",
    "        print(f\"\\nğŸ“… Tendencia temporal ({time_col}):\")\n",
    "        if 'year' in time_col.lower():\n",
    "            time_stats = df_processed.groupby(time_col)[value_col].agg(['count', 'mean']).round(0)\n",
    "            display(time_stats.tail(10))  # Ãšltimos 10 aÃ±os\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "else:\n",
    "    print(\"âš ï¸ Datos procesados no disponibles para anÃ¡lisis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34abc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DetecciÃ³n de outliers y anÃ¡lisis de calidad\n",
    "if 'df_processed' in locals():\n",
    "    print(\"ğŸ¯ DETECCIÃ“N DE OUTLIERS Y CALIDAD DE DATOS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    value_col = 'value' if 'value' in df_processed.columns else df_processed.select_dtypes(include=[np.number]).columns[0]\n",
    "    \n",
    "    # MÃ©todo IQR para detecciÃ³n de outliers\n",
    "    Q1 = df_processed[value_col].quantile(0.25)\n",
    "    Q3 = df_processed[value_col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers_iqr = df_processed[(df_processed[value_col] < lower_bound) | \n",
    "                               (df_processed[value_col] > upper_bound)]\n",
    "    \n",
    "    print(f\"ğŸ“Š DetecciÃ³n de outliers (MÃ©todo IQR):\")\n",
    "    print(f\"   Rango normal: {lower_bound:,.0f} - {upper_bound:,.0f}\")\n",
    "    print(f\"   Outliers encontrados: {len(outliers_iqr)} ({len(outliers_iqr)/len(df_processed)*100:.2f}%)\")\n",
    "    \n",
    "    # MÃ©todo Z-Score para outliers extremos\n",
    "    z_scores = np.abs(stats.zscore(df_processed[value_col]))\n",
    "    outliers_zscore = df_processed[z_scores > 3]\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Outliers extremos (Z-score > 3):\")\n",
    "    print(f\"   Outliers extremos: {len(outliers_zscore)} ({len(outliers_zscore)/len(df_processed)*100:.2f}%)\")\n",
    "    \n",
    "    if len(outliers_zscore) > 0:\n",
    "        print(f\"   Valores extremos:\")\n",
    "        extreme_values = outliers_zscore[value_col].sort_values(ascending=False)\n",
    "        for i, (idx, val) in enumerate(extreme_values.head(5).items()):\n",
    "            print(f\"      {i+1}. {val:,.0f}\")\n",
    "    \n",
    "    # AnÃ¡lisis de distribuciÃ³n\n",
    "    print(f\"\\nğŸ“ˆ AnÃ¡lisis de distribuciÃ³n:\")\n",
    "    \n",
    "    # Test de normalidad (Shapiro-Wilk para muestras pequeÃ±as, Anderson-Darling para grandes)\n",
    "    if len(df_processed) <= 5000:\n",
    "        shapiro_stat, shapiro_p = stats.shapiro(df_processed[value_col].dropna().sample(min(5000, len(df_processed))))\n",
    "        print(f\"   Test de Shapiro-Wilk: estadÃ­stico={shapiro_stat:.4f}, p-valor={shapiro_p:.6f}\")\n",
    "        is_normal = shapiro_p > 0.05\n",
    "    else:\n",
    "        # Para muestras grandes, usar test de Anderson-Darling\n",
    "        anderson_result = stats.anderson(df_processed[value_col].dropna())\n",
    "        print(f\"   Test de Anderson-Darling: estadÃ­stico={anderson_result.statistic:.4f}\")\n",
    "        is_normal = anderson_result.statistic < anderson_result.critical_values[2]  # 5% level\n",
    "    \n",
    "    print(f\"   Â¿DistribuciÃ³n normal?: {'SÃ­' if is_normal else 'No'}\")\n",
    "    \n",
    "    # Skewness y Kurtosis\n",
    "    skewness = stats.skew(df_processed[value_col].dropna())\n",
    "    kurtosis = stats.kurtosis(df_processed[value_col].dropna())\n",
    "    \n",
    "    print(f\"   AsimetrÃ­a (skewness): {skewness:.3f}\")\n",
    "    print(f\"   Curtosis (kurtosis): {kurtosis:.3f}\")\n",
    "    \n",
    "    # InterpretaciÃ³n\n",
    "    if abs(skewness) < 0.5:\n",
    "        skew_interp = \"aproximadamente simÃ©trica\"\n",
    "    elif abs(skewness) < 1:\n",
    "        skew_interp = \"moderadamente asimÃ©trica\"\n",
    "    else:\n",
    "        skew_interp = \"altamente asimÃ©trica\"\n",
    "    \n",
    "    print(f\"   InterpretaciÃ³n: DistribuciÃ³n {skew_interp}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "else:\n",
    "    print(\"âš ï¸ Datos procesados no disponibles para anÃ¡lisis de outliers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efe98cb",
   "metadata": {},
   "source": [
    "## 5. ğŸ“Š VisualizaciÃ³n Profesional de la Fuerza de Trabajo\n",
    "\n",
    "CreaciÃ³n de grÃ¡ficos ejecutivos y dashboards interactivos para comunicar los hallazgos de manera clara y profesional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff26fa79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizaciones bÃ¡sicas con matplotlib y seaborn\n",
    "if 'df_processed' in locals():\n",
    "    print(\"ğŸ“ˆ CREANDO VISUALIZACIONES PROFESIONALES\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    value_col = 'value' if 'value' in df_processed.columns else df_processed.select_dtypes(include=[np.number]).columns[0]\n",
    "    \n",
    "    # ConfiguraciÃ³n de estilo\n",
    "    plt.style.use('seaborn-v0_8')\n",
    "    fig_size = (15, 10)\n",
    "    \n",
    "    # 1. DistribuciÃ³n de la variable principal\n",
    "    plt.figure(figsize=fig_size)\n",
    "    \n",
    "    # Subplot 1: Histograma\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.hist(df_processed[value_col] / 1000, bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    plt.title('DistribuciÃ³n de la Fuerza de Trabajo', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Miles de personas')\n",
    "    plt.ylabel('Frecuencia')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Subplot 2: Box plot\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.boxplot(df_processed[value_col] / 1000)\n",
    "    plt.title('Diagrama de Caja', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('Miles de personas')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Subplot 3: Q-Q Plot\n",
    "    plt.subplot(2, 2, 3)\n",
    "    stats.probplot(df_processed[value_col].dropna(), dist=\"norm\", plot=plt)\n",
    "    plt.title('Q-Q Plot (Test de Normalidad)', fontsize=14, fontweight='bold')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Subplot 4: AnÃ¡lisis temporal (si existe)\n",
    "    plt.subplot(2, 2, 4)\n",
    "    \n",
    "    # Buscar columna de tiempo\n",
    "    time_cols = [col for col in df_processed.columns if any(x in col.lower() for x in ['date', 'year', 'fecha', 'aÃ±o'])]\\n    \n",
    "    if time_cols:\n",
    "        time_col = time_cols[0]\n",
    "        \n",
    "        # Agrupar por tiempo y calcular promedio\n",
    "        if 'date' in time_col.lower():\n",
    "            time_data = df_processed.groupby(time_col)[value_col].mean() / 1000\n",
    "        else:\n",
    "            time_data = df_processed.groupby(time_col)[value_col].mean() / 1000\n",
    "        \n",
    "        plt.plot(time_data.index, time_data.values, marker='o', linewidth=2, color='coral')\n",
    "        plt.title('EvoluciÃ³n Temporal', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Tiempo')\n",
    "        plt.ylabel('Miles de personas (promedio)')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, 'No hay datos temporales\\\\ndisponibles', \n",
    "                ha='center', va='center', transform=plt.gca().transAxes,\n",
    "                fontsize=12, bbox=dict(boxstyle='round', facecolor='lightgray'))\n",
    "        plt.title('EvoluciÃ³n Temporal', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"âœ… Visualizaciones bÃ¡sicas completadas\")\n",
    "else:\n",
    "    print(\"âš ï¸ Datos no disponibles para visualizaciÃ³n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ffd938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizaciones por dimensiones (regiÃ³n, gÃ©nero, tiempo)\n",
    "if 'df_processed' in locals():\n",
    "    print(\"ğŸŒ ANÃLISIS POR DIMENSIONES\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    value_col = 'value' if 'value' in df_processed.columns else df_processed.select_dtypes(include=[np.number]).columns[0]\n",
    "    \n",
    "    # AnÃ¡lisis por regiÃ³n\n",
    "    region_cols = [col for col in df_processed.columns if 'region' in col.lower()]\n",
    "    if region_cols:\n",
    "        region_col = region_cols[0]\n",
    "        \n",
    "        # Top 10 regiones por fuerza de trabajo promedio\n",
    "        plt.figure(figsize=(14, 8))\n",
    "        \n",
    "        region_avg = df_processed.groupby(region_col)[value_col].mean().sort_values(ascending=True) / 1000\n",
    "        top_regions = region_avg.tail(10)\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        bars = plt.barh(range(len(top_regions)), top_regions.values, color='lightcoral')\n",
    "        plt.yticks(range(len(top_regions)), top_regions.index)\n",
    "        plt.title('Top 10 Regiones\\\\n(Fuerza de Trabajo Promedio)', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Miles de personas')\n",
    "        plt.grid(True, alpha=0.3, axis='x')\n",
    "        \n",
    "        # AÃ±adir valores en las barras\n",
    "        for i, bar in enumerate(bars):\n",
    "            width = bar.get_width()\n",
    "            plt.text(width + width*0.01, bar.get_y() + bar.get_height()/2, \n",
    "                    f'{width:.0f}', ha='left', va='center', fontsize=10)\n",
    "        \n",
    "        # DistribuciÃ³n de varianza regional\n",
    "        plt.subplot(1, 2, 2)\n",
    "        region_std = df_processed.groupby(region_col)[value_col].std() / 1000\n",
    "        plt.scatter(region_avg.values, region_std.values, alpha=0.6, s=60, color='darkblue')\n",
    "        plt.xlabel('Promedio (miles de personas)')\n",
    "        plt.ylabel('DesviaciÃ³n estÃ¡ndar (miles)')\n",
    "        plt.title('Variabilidad Regional\\\\n(Promedio vs Varianza)', fontsize=14, fontweight='bold')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # AÃ±adir lÃ­nea de tendencia\n",
    "        z = np.polyfit(region_avg.values, region_std.values, 1)\n",
    "        p = np.poly1d(z)\n",
    "        plt.plot(region_avg.values, p(region_avg.values), \\\"r--\\\", alpha=0.8)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # AnÃ¡lisis por gÃ©nero\n",
    "    gender_cols = [col for col in df_processed.columns if any(x in col.lower() for x in ['sexo', 'gender'])]\n",
    "    if gender_cols:\n",
    "        gender_col = gender_cols[0]\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # ComparaciÃ³n por gÃ©nero\n",
    "        plt.subplot(1, 2, 1)\n",
    "        gender_data = df_processed.groupby(gender_col)[value_col].mean() / 1000\n",
    "        colors = ['lightblue', 'lightpink', 'lightgreen'][:len(gender_data)]\n",
    "        \n",
    "        bars = plt.bar(gender_data.index, gender_data.values, color=colors, alpha=0.8, edgecolor='black')\n",
    "        plt.title('Fuerza de Trabajo por GÃ©nero\\\\n(Promedio)', fontsize=14, fontweight='bold')\n",
    "        plt.ylabel('Miles de personas')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # AÃ±adir valores en las barras\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            plt.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "                    f'{height:.0f}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "        \n",
    "        # Box plot por gÃ©nero\n",
    "        plt.subplot(1, 2, 2)\n",
    "        gender_groups = [df_processed[df_processed[gender_col] == gender][value_col] / 1000 \n",
    "                        for gender in df_processed[gender_col].unique()]\n",
    "        \n",
    "        plt.boxplot(gender_groups, labels=df_processed[gender_col].unique())\n",
    "        plt.title('DistribuciÃ³n por GÃ©nero', fontsize=14, fontweight='bold')\n",
    "        plt.ylabel('Miles de personas')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    print(\"âœ… AnÃ¡lisis dimensional completado\")\n",
    "else:\n",
    "    print(\"âš ï¸ Datos no disponibles para anÃ¡lisis dimensional\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0969b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dashboard interactivo con Plotly\n",
    "if 'df_processed' in locals():\n",
    "    print(\"ğŸš€ DASHBOARD INTERACTIVO\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    value_col = 'value' if 'value' in df_processed.columns else df_processed.select_dtypes(include=[np.number]).columns[0]\n",
    "    \n",
    "    try:\n",
    "        # Crear dashboard usando mÃ³dulo personalizado\n",
    "        chart_maker = LabourForceCharts()\n",
    "        interactive_fig = chart_maker.create_interactive_dashboard(\n",
    "            df_processed, \n",
    "            title=\"Dashboard Interactivo - Fuerza de Trabajo Chile\"\n",
    "        )\n",
    "        interactive_fig.show()\n",
    "        print(\"âœ… Dashboard interactivo creado con mÃ³dulo personalizado\")\n",
    "        \n",
    "    except NameError:\n",
    "        print(\"âš ï¸ Creando dashboard bÃ¡sico (mÃ³dulo personalizado no disponible)\")\n",
    "        \n",
    "        # Dashboard bÃ¡sico con Plotly\n",
    "        from plotly.subplots import make_subplots\n",
    "        \n",
    "        fig = make_subplots(\n",
    "            rows=2, cols=2,\n",
    "            subplot_titles=('DistribuciÃ³n General', 'EvoluciÃ³n Temporal', \n",
    "                          'Por RegiÃ³n (Top 10)', 'Por GÃ©nero'),\n",
    "            specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    "                   [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
    "        )\n",
    "        \n",
    "        # 1. Histograma de distribuciÃ³n\n",
    "        fig.add_trace(\n",
    "            go.Histogram(x=df_processed[value_col] / 1000, \n",
    "                        name='DistribuciÃ³n', \n",
    "                        nbinsx=30,\n",
    "                        marker_color='lightblue'),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        # 2. EvoluciÃ³n temporal (si existe)\n",
    "        time_cols = [col for col in df_processed.columns if any(x in col.lower() for x in ['date', 'year', 'fecha', 'aÃ±o'])]\n",
    "        if time_cols:\n",
    "            time_col = time_cols[0]\n",
    "            time_data = df_processed.groupby(time_col)[value_col].mean() / 1000\n",
    "            \n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=time_data.index, y=time_data.values,\n",
    "                          mode='lines+markers',\n",
    "                          name='EvoluciÃ³n',\n",
    "                          line=dict(color='coral', width=2)),\n",
    "                row=1, col=2\n",
    "            )\n",
    "        \n",
    "        # 3. Top regiones (si existe)\n",
    "        region_cols = [col for col in df_processed.columns if 'region' in col.lower()]\n",
    "        if region_cols:\n",
    "            region_col = region_cols[0]\n",
    "            region_data = df_processed.groupby(region_col)[value_col].mean().sort_values(ascending=True).tail(10) / 1000\n",
    "            \n",
    "            fig.add_trace(\n",
    "                go.Bar(y=region_data.index, x=region_data.values,\n",
    "                      orientation='h',\n",
    "                      name='Regiones',\n",
    "                      marker_color='lightgreen'),\n",
    "                row=2, col=1\n",
    "            )\n",
    "        \n",
    "        # 4. Por gÃ©nero (si existe)\n",
    "        gender_cols = [col for col in df_processed.columns if any(x in col.lower() for x in ['sexo', 'gender'])]\n",
    "        if gender_cols:\n",
    "            gender_col = gender_cols[0]\n",
    "            gender_data = df_processed.groupby(gender_col)[value_col].mean() / 1000\n",
    "            \n",
    "            fig.add_trace(\n",
    "                go.Bar(x=gender_data.index, y=gender_data.values,\n",
    "                      name='GÃ©nero',\n",
    "                      marker_color='lightcoral'),\n",
    "                row=2, col=2\n",
    "            )\n",
    "        \n",
    "        # Configurar layout\n",
    "        fig.update_layout(\n",
    "            title_text=\"Dashboard Fuerza de Trabajo Chile\",\n",
    "            height=800,\n",
    "            showlegend=False,\n",
    "            template='plotly_white'\n",
    "        )\n",
    "        \n",
    "        # Actualizar etiquetas de ejes\n",
    "        fig.update_xaxes(title_text=\"Miles de personas\", row=1, col=1)\n",
    "        fig.update_yaxes(title_text=\"Frecuencia\", row=1, col=1)\n",
    "        \n",
    "        if time_cols:\n",
    "            fig.update_xaxes(title_text=\"Tiempo\", row=1, col=2)\n",
    "            fig.update_yaxes(title_text=\"Miles de personas\", row=1, col=2)\n",
    "        \n",
    "        if region_cols:\n",
    "            fig.update_xaxes(title_text=\"Miles de personas\", row=2, col=1)\n",
    "            fig.update_yaxes(title_text=\"RegiÃ³n\", row=2, col=1)\n",
    "        \n",
    "        if gender_cols:\n",
    "            fig.update_xaxes(title_text=\"GÃ©nero\", row=2, col=2)\n",
    "            fig.update_yaxes(title_text=\"Miles de personas\", row=2, col=2)\n",
    "        \n",
    "        fig.show()\n",
    "        print(\"âœ… Dashboard bÃ¡sico completado\")\n",
    "    \n",
    "    print(\"\\nğŸ’¡ El dashboard es interactivo: puedes hacer zoom, filtrar y explorar los datos\")\n",
    "    print(\"=\" * 50)\n",
    "else:\n",
    "    print(\"âš ï¸ Datos no disponibles para dashboard\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c9dcac",
   "metadata": {},
   "source": [
    "## 6. ğŸ§  GeneraciÃ³n de Insights y Modelos Predictivos\n",
    "\n",
    "IdentificaciÃ³n de patrones clave, generaciÃ³n de insights econÃ³micos y construcciÃ³n de modelos predictivos cuando los datos lo permiten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e7d851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GeneraciÃ³n de insights principales\n",
    "if 'df_processed' in locals():\n",
    "    print(\"ğŸ’¡ INSIGHTS ECONÃ“MICOS PRINCIPALES\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    value_col = 'value' if 'value' in df_processed.columns else df_processed.select_dtypes(include=[np.number]).columns[0]\n",
    "    insights = []\n",
    "    \n",
    "    # 1. Insight general sobre magnitud\n",
    "    total_workforce = df_processed[value_col].sum()\n",
    "    avg_workforce = df_processed[value_col].mean()\n",
    "    \n",
    "    insights.append(f\"ğŸ“Š La fuerza de trabajo total analizada alcanza {total_workforce/1000000:.1f} millones de personas\")\n",
    "    insights.append(f\"ğŸ“ˆ El promedio por observaciÃ³n es de {avg_workforce/1000:.0f} mil personas\")\n",
    "    \n",
    "    # 2. Insights temporales\n",
    "    time_cols = [col for col in df_processed.columns if any(x in col.lower() for x in ['date', 'year', 'fecha', 'aÃ±o'])]\n",
    "    if time_cols:\n",
    "        time_col = time_cols[0]\n",
    "        \n",
    "        # Calcular tendencia temporal\n",
    "        time_data = df_processed.groupby(time_col)[value_col].sum()\n",
    "        \n",
    "        if len(time_data) > 2:\n",
    "            # Calcular tasa de crecimiento\n",
    "            first_period = time_data.iloc[0]\n",
    "            last_period = time_data.iloc[-1]\n",
    "            periods = len(time_data) - 1\n",
    "            \n",
    "            growth_rate = ((last_period / first_period) ** (1/periods) - 1) * 100\n",
    "            total_growth = ((last_period / first_period) - 1) * 100\n",
    "            \n",
    "            insights.append(f\"ğŸ“… Tendencia temporal: crecimiento promedio anual del {growth_rate:.2f}%\")\n",
    "            insights.append(f\"ğŸ“ˆ Crecimiento total en el perÃ­odo: {total_growth:.1f}%\")\n",
    "            \n",
    "            # Identificar perÃ­odo de mayor/menor actividad\n",
    "            max_period = time_data.idxmax()\n",
    "            min_period = time_data.idxmin()\n",
    "            \n",
    "            insights.append(f\"ğŸ” PerÃ­odo de mayor actividad: {max_period} ({time_data[max_period]/1000000:.2f}M personas)\")\n",
    "            insights.append(f\"ğŸ”» PerÃ­odo de menor actividad: {min_period} ({time_data[min_period]/1000000:.2f}M personas)\")\n",
    "    \n",
    "    # 3. Insights regionales\n",
    "    region_cols = [col for col in df_processed.columns if 'region' in col.lower()]\n",
    "    if region_cols:\n",
    "        region_col = region_cols[0]\n",
    "        \n",
    "        region_totals = df_processed.groupby(region_col)[value_col].sum().sort_values(ascending=False)\n",
    "        \n",
    "        # RegiÃ³n lÃ­der\n",
    "        top_region = region_totals.index[0]\n",
    "        top_value = region_totals.iloc[0]\n",
    "        \n",
    "        # ConcentraciÃ³n regional\n",
    "        top_3_share = region_totals.head(3).sum() / region_totals.sum() * 100\n",
    "        \n",
    "        insights.append(f\"ğŸŒ RegiÃ³n lÃ­der: {top_region} ({top_value/1000000:.1f}M personas)\")\n",
    "        insights.append(f\"ğŸ¯ Las top 3 regiones concentran el {top_3_share:.1f}% de la fuerza laboral\")\n",
    "        \n",
    "        # Disparidad regional\n",
    "        region_cv = (region_totals.std() / region_totals.mean()) * 100\n",
    "        insights.append(f\"ğŸ“Š Disparidad regional (CV): {region_cv:.1f}% ({'Alta' if region_cv > 50 else 'Moderada' if region_cv > 30 else 'Baja'})\")\n",
    "    \n",
    "    # 4. Insights de gÃ©nero\n",
    "    gender_cols = [col for col in df_processed.columns if any(x in col.lower() for x in ['sexo', 'gender'])]\n",
    "    if gender_cols:\n",
    "        gender_col = gender_cols[0]\n",
    "        \n",
    "        gender_totals = df_processed.groupby(gender_col)[value_col].sum()\n",
    "        \n",
    "        if 'M' in gender_totals.index and 'F' in gender_totals.index:\n",
    "            male_total = gender_totals.get('M', 0)\n",
    "            female_total = gender_totals.get('F', 0)\n",
    "            \n",
    "            if male_total > 0 and female_total > 0:\n",
    "                gender_ratio = male_total / female_total\n",
    "                female_participation = female_total / (male_total + female_total) * 100\n",
    "                \n",
    "                insights.append(f\"ğŸ‘¥ ParticipaciÃ³n femenina: {female_participation:.1f}% del total\")\n",
    "                insights.append(f\"âš–ï¸ Ratio hombres/mujeres: {gender_ratio:.2f}\")\n",
    "                \n",
    "                if gender_ratio > 1.5:\n",
    "                    insights.append(\"ğŸ“Š Existe una brecha significativa de gÃ©nero en participaciÃ³n laboral\")\n",
    "                elif gender_ratio < 1.1:\n",
    "                    insights.append(\"ğŸ“Š La participaciÃ³n laboral estÃ¡ relativamente equilibrada por gÃ©nero\")\n",
    "    \n",
    "    # 5. Insights de variabilidad\n",
    "    cv = (df_processed[value_col].std() / df_processed[value_col].mean()) * 100\n",
    "    insights.append(f\"ğŸ“ˆ Variabilidad general (CV): {cv:.1f}% ({'Alta' if cv > 30 else 'Moderada' if cv > 15 else 'Baja'})\")\n",
    "    \n",
    "    # Mostrar insights\n",
    "    print(\"ğŸ” PRINCIPALES HALLAZGOS:\")\n",
    "    for i, insight in enumerate(insights, 1):\n",
    "        print(f\"{i:2d}. {insight}\")\n",
    "    \n",
    "    print(\"\\\\n\" + \"=\"*60)\n",
    "    \n",
    "    # Generar recomendaciones\n",
    "    print(\"\\\\nğŸ¯ RECOMENDACIONES ESTRATÃ‰GICAS:\")\n",
    "    recommendations = []\n",
    "    \n",
    "    if 'growth_rate' in locals() and growth_rate > 2:\n",
    "        recommendations.append(\"ğŸ“ˆ El crecimiento sostenido sugiere polÃ­ticas de empleo efectivas\")\n",
    "    elif 'growth_rate' in locals() and growth_rate < 0:\n",
    "        recommendations.append(\"âš ï¸ El decrecimiento requiere polÃ­ticas de estÃ­mulo al empleo\")\n",
    "    \n",
    "    if 'region_cv' in locals() and region_cv > 50:\n",
    "        recommendations.append(\"ğŸŒ Alta disparidad regional requiere polÃ­ticas de desarrollo territorial\")\n",
    "    \n",
    "    if 'female_participation' in locals() and female_participation < 40:\n",
    "        recommendations.append(\"ğŸ‘© Baja participaciÃ³n femenina sugiere necesidad de polÃ­ticas de inclusiÃ³n\")\n",
    "    \n",
    "    if cv > 30:\n",
    "        recommendations.append(\"ğŸ“Š Alta variabilidad indica mercado laboral heterogÃ©neo\")\n",
    "    \n",
    "    if not recommendations:\n",
    "        recommendations.append(\"ğŸ“Š Los indicadores muestran un mercado laboral relativamente estable\")\n",
    "    \n",
    "    for i, rec in enumerate(recommendations, 1):\n",
    "        print(f\"{i}. {rec}\")\n",
    "    \n",
    "    print(\"\\\\n\" + \"=\"*60)\n",
    "else:\n",
    "    print(\"âš ï¸ Datos no disponibles para generaciÃ³n de insights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80705381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo predictivo bÃ¡sico (si hay datos temporales)\n",
    "if 'df_processed' in locals():\n",
    "    print(\"ğŸ¤– MODELO PREDICTIVO BÃSICO\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    time_cols = [col for col in df_processed.columns if any(x in col.lower() for x in ['date', 'year', 'fecha', 'aÃ±o'])]\n",
    "    value_col = 'value' if 'value' in df_processed.columns else df_processed.select_dtypes(include=[np.number]).columns[0]\n",
    "    \n",
    "    if time_cols:\n",
    "        time_col = time_cols[0]\n",
    "        \n",
    "        # Preparar datos para modelado\n",
    "        # Filtrar datos nacionales para el modelo\n",
    "        national_filter = df_processed['region_code'] == '_T' if 'region_code' in df_processed.columns else True\n",
    "        gender_filter = df_processed['gender_code'] == '_T' if 'gender_code' in df_processed.columns else True\n",
    "        \n",
    "        if isinstance(national_filter, bool):\n",
    "            model_data = df_processed[gender_filter] if not isinstance(gender_filter, bool) else df_processed\n",
    "        else:\n",
    "            model_data = df_processed[national_filter & gender_filter] if not isinstance(gender_filter, bool) else df_processed[national_filter]\n",
    "        \n",
    "        if len(model_data) > 5:  # Necesitamos suficientes datos\n",
    "            # Crear series temporal\n",
    "            ts_data = model_data.groupby(time_col)[value_col].mean().sort_index()\n",
    "            \n",
    "            print(f\"ğŸ“Š Datos para modelado: {len(ts_data)} observaciones temporales\")\n",
    "            \n",
    "            # Modelo de tendencia lineal simple\n",
    "            from sklearn.linear_model import LinearRegression\n",
    "            from sklearn.metrics import mean_absolute_error, r2_score\n",
    "            \n",
    "            # Preparar variables\n",
    "            X = np.arange(len(ts_data)).reshape(-1, 1)  # Tiempo como variable numÃ©rica\n",
    "            y = ts_data.values\n",
    "            \n",
    "            # Dividir en entrenamiento y prueba (80-20)\n",
    "            split_idx = int(len(X) * 0.8)\n",
    "            X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "            y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "            \n",
    "            # Entrenar modelo\n",
    "            model = LinearRegression()\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            # Predicciones\n",
    "            y_pred_train = model.predict(X_train)\n",
    "            y_pred_test = model.predict(X_test) if len(X_test) > 0 else []\n",
    "            \n",
    "            # MÃ©tricas\n",
    "            train_r2 = r2_score(y_train, y_pred_train)\n",
    "            train_mae = mean_absolute_error(y_train, y_pred_train)\n",
    "            \n",
    "            print(f\"âœ… Modelo entrenado exitosamente\")\n",
    "            print(f\"ğŸ“ˆ RÂ² (entrenamiento): {train_r2:.3f}\")\n",
    "            print(f\"ğŸ“Š MAE (entrenamiento): {train_mae/1000:.0f} miles de personas\")\n",
    "            \n",
    "            if len(y_test) > 0:\n",
    "                test_r2 = r2_score(y_test, y_pred_test)\n",
    "                test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "                print(f\"ğŸ¯ RÂ² (prueba): {test_r2:.3f}\")\n",
    "                print(f\"ğŸ“Š MAE (prueba): {test_mae/1000:.0f} miles de personas\")\n",
    "            \n",
    "            # ProyecciÃ³n futura (prÃ³ximos 4 perÃ­odos)\n",
    "            future_periods = 4\n",
    "            X_future = np.arange(len(ts_data), len(ts_data) + future_periods).reshape(-1, 1)\n",
    "            y_future = model.predict(X_future)\n",
    "            \n",
    "            print(f\"\\\\nğŸ”® PROYECCIONES FUTURAS ({future_periods} perÃ­odos):\")\n",
    "            for i, pred in enumerate(y_future, 1):\n",
    "                print(f\"   PerÃ­odo +{i}: {pred/1000:.0f} miles de personas\")\n",
    "            \n",
    "            # VisualizaciÃ³n del modelo\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            \n",
    "            # Datos histÃ³ricos\n",
    "            plt.plot(range(len(ts_data)), ts_data.values/1000, 'o-', label='Datos reales', color='blue', alpha=0.7)\n",
    "            \n",
    "            # Predicciones del modelo\n",
    "            plt.plot(range(len(y_train)), y_pred_train/1000, '--', label='PredicciÃ³n (entrenamiento)', color='red', alpha=0.8)\n",
    "            \n",
    "            if len(y_test) > 0:\n",
    "                plt.plot(range(len(y_train), len(ts_data)), y_pred_test/1000, '--', label='PredicciÃ³n (prueba)', color='orange', alpha=0.8)\n",
    "            \n",
    "            # Proyecciones futuras\n",
    "            future_x = range(len(ts_data), len(ts_data) + future_periods)\n",
    "            plt.plot(future_x, y_future/1000, 's--', label='ProyecciÃ³n futura', color='green', alpha=0.8)\n",
    "            \n",
    "            # ConfiguraciÃ³n del grÃ¡fico\n",
    "            plt.title('Modelo Predictivo de Fuerza de Trabajo', fontsize=14, fontweight='bold')\n",
    "            plt.xlabel('PerÃ­odo')\n",
    "            plt.ylabel('Miles de personas')\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            \n",
    "            # AÃ±adir lÃ­nea divisoria entre datos y proyecciÃ³n\n",
    "            plt.axvline(x=len(ts_data)-0.5, color='gray', linestyle=':', alpha=0.7, label='Inicio proyecciÃ³n')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # InterpretaciÃ³n del modelo\n",
    "            slope = model.coef_[0]\n",
    "            trend_interpretation = \"creciente\" if slope > 0 else \"decreciente\" if slope < 0 else \"estable\"\n",
    "            \n",
    "            print(f\"\\\\nğŸ“Š INTERPRETACIÃ“N DEL MODELO:\")\n",
    "            print(f\"   Tendencia: {trend_interpretation}\")\n",
    "            print(f\"   Cambio promedio por perÃ­odo: {slope/1000:+.0f} miles de personas\")\n",
    "            \n",
    "            if train_r2 > 0.8:\n",
    "                print(f\"   Calidad del modelo: Excelente (RÂ² = {train_r2:.3f})\")\n",
    "            elif train_r2 > 0.6:\n",
    "                print(f\"   Calidad del modelo: Buena (RÂ² = {train_r2:.3f})\")\n",
    "            elif train_r2 > 0.4:\n",
    "                print(f\"   Calidad del modelo: Moderada (RÂ² = {train_r2:.3f})\")\n",
    "            else:\n",
    "                print(f\"   Calidad del modelo: Baja (RÂ² = {train_r2:.3f}) - datos muy variables\")\n",
    "            \n",
    "        else:\n",
    "            print(\"âš ï¸ Insuficientes datos temporales para modelado\")\n",
    "    else:\n",
    "        print(\"âš ï¸ No se encontraron columnas temporales para modelado predictivo\")\n",
    "        print(\"ğŸ’¡ Sugerencia: Incluir variables temporales para habilitar predicciones\")\n",
    "    \n",
    "    print(\"\\\\n\" + \"=\"*50)\n",
    "else:\n",
    "    print(\"âš ï¸ Datos no disponibles para modelado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d8a695",
   "metadata": {},
   "source": [
    "## 7. ğŸ—ï¸ Buenas PrÃ¡cticas y Estructura Modular del Proyecto\n",
    "\n",
    "DemostraciÃ³n de la organizaciÃ³n profesional del cÃ³digo, aplicaciÃ³n de principios SOLID y Clean Code, y documentaciÃ³n de la arquitectura del proyecto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5539019e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DocumentaciÃ³n de la estructura del proyecto\n",
    "print(\"ğŸ—ï¸ ESTRUCTURA MODULAR DEL PROYECTO\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\"\"\n",
    "ğŸ“ ine-chile-labour-force-analysis/\n",
    "â”œâ”€â”€ ğŸ“Š data/\n",
    "â”‚   â”œâ”€â”€ raw/           # Datos crudos del INE (CSV originales)\n",
    "â”‚   â”œâ”€â”€ processed/     # Datos limpios y transformados\n",
    "â”‚   â””â”€â”€ outputs/       # Resultados finales y exportaciones\n",
    "â”œâ”€â”€ ğŸ“ src/           # CÃ³digo fuente modular\n",
    "â”‚   â”œâ”€â”€ etl/          # Extract, Transform, Load\n",
    "â”‚   â”‚   â”œâ”€â”€ __init__.py\n",
    "â”‚   â”‚   â”œâ”€â”€ base.py           # Clases base abstractas\n",
    "â”‚   â”‚   â””â”€â”€ data_processor.py # Procesador de datos del INE\n",
    "â”‚   â”œâ”€â”€ models/       # Modelos estadÃ­sticos y ML\n",
    "â”‚   â”‚   â””â”€â”€ __init__.py\n",
    "â”‚   â”œâ”€â”€ utils/        # Utilidades y helpers\n",
    "â”‚   â”‚   â”œâ”€â”€ __init__.py\n",
    "â”‚   â”‚   â”œâ”€â”€ logger_config.py  # ConfiguraciÃ³n de logging\n",
    "â”‚   â”‚   â””â”€â”€ validators.py     # Validadores de datos\n",
    "â”‚   â””â”€â”€ visualization/ # GrÃ¡ficos y dashboards\n",
    "â”‚       â”œâ”€â”€ __init__.py\n",
    "â”‚       â””â”€â”€ charts.py         # Clases para visualizaciÃ³n\n",
    "â”œâ”€â”€ ğŸ“” notebooks/     # Jupyter notebooks de anÃ¡lisis\n",
    "â”‚   â””â”€â”€ 01_eda_labour_force.ipynb\n",
    "â”œâ”€â”€ ğŸ”§ scripts/       # Scripts de automatizaciÃ³n\n",
    "â”œâ”€â”€ ğŸ§ª tests/         # Tests unitarios e integraciÃ³n\n",
    "â”œâ”€â”€ ğŸ“š docs/          # DocumentaciÃ³n del proyecto\n",
    "â”œâ”€â”€ ğŸ“‹ config.py      # ConfiguraciÃ³n centralizada\n",
    "â”œâ”€â”€ ğŸ“„ requirements.txt # Dependencias\n",
    "â”œâ”€â”€ âš™ï¸ setup.py       # ConfiguraciÃ³n del paquete\n",
    "â””â”€â”€ ğŸ“– README.md      # DocumentaciÃ³n principal\n",
    "\"\"\")\n",
    "\n",
    "print(\"ğŸ¯ PRINCIPIOS APLICADOS:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "principles = [\n",
    "    (\"Clean Code\", [\n",
    "        \"Nombres descriptivos y funciones pequeÃ±as\",\n",
    "        \"Principio DRY (Don't Repeat Yourself)\", \n",
    "        \"Comentarios claros y documentaciÃ³n exhaustiva\",\n",
    "        \"CÃ³digo auto-documentado y legible\"\n",
    "    ]),\n",
    "    \n",
    "    (\"SOLID Principles\", [\n",
    "        \"S - Single Responsibility: Cada clase tiene una responsabilidad\",\n",
    "        \"O - Open/Closed: Abierto para extensiÃ³n, cerrado para modificaciÃ³n\",\n",
    "        \"L - Liskov Substitution: Clases derivadas son sustituibles\",\n",
    "        \"I - Interface Segregation: Interfaces especÃ­ficas y cohesivas\",\n",
    "        \"D - Dependency Inversion: Depender de abstracciones, no concreciones\"\n",
    "    ]),\n",
    "    \n",
    "    (\"Data Science Best Practices\", [\n",
    "        \"ValidaciÃ³n exhaustiva de datos\",\n",
    "        \"Logging y monitoreo de procesos\",\n",
    "        \"Reproducibilidad de experimentos\",\n",
    "        \"Versionado de datos y cÃ³digo\",\n",
    "        \"DocumentaciÃ³n de metodologÃ­a\"\n",
    "    ])\n",
    "]\n",
    "\n",
    "for principle, practices in principles:\n",
    "    print(f\"\\\\nğŸ“š {principle}:\")\n",
    "    for practice in practices:\n",
    "        print(f\"   âœ… {practice}\")\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f443c2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de uso de mÃ³dulos profesionales y guardar resultados\n",
    "print(\"ğŸ’¾ GUARDADO DE RESULTADOS Y EJEMPLO DE MODULARIEDAD\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Guardar datos procesados\n",
    "if 'df_processed' in locals():\n",
    "    \n",
    "    # Crear directorio de salida si no existe\n",
    "    output_path = processed_data_path / \"labour_force_processed.csv\"\n",
    "    \n",
    "    try:\n",
    "        # Guardar datos procesados\n",
    "        df_processed.to_csv(output_path, index=False, encoding='utf-8')\n",
    "        print(f\"âœ… Datos procesados guardados en: {output_path}\")\n",
    "        print(f\"ğŸ“Š Registros guardados: {len(df_processed):,}\")\n",
    "        \n",
    "        # Guardar resumen estadÃ­stico\n",
    "        summary_path = output_data_path / \"summary_statistics.txt\"\n",
    "        \n",
    "        with open(summary_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(\"RESUMEN ESTADÃSTICO - FUERZA DE TRABAJO CHILE\\\\n\")\n",
    "            f.write(\"=\" * 50 + \"\\\\n\\\\n\")\n",
    "            \n",
    "            f.write(f\"Fecha de anÃ¡lisis: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\\\\n\")\n",
    "            f.write(f\"Total de registros: {len(df_processed):,}\\\\n\")\n",
    "            f.write(f\"PerÃ­odo analizado: {df_processed['date'].min()} a {df_processed['date'].max()}\\\\n\" if 'date' in df_processed.columns else \"\")\n",
    "            f.write(f\"Regiones incluidas: {df_processed['region_code'].nunique()}\\\\n\" if 'region_code' in df_processed.columns else \"\")\n",
    "            \n",
    "            # EstadÃ­sticas principales\n",
    "            value_col = 'value' if 'value' in df_processed.columns else df_processed.select_dtypes(include=[np.number]).columns[0]\n",
    "            stats = df_processed[value_col].describe()\n",
    "            \n",
    "            f.write(\"\\\\nESTADÃSTICAS PRINCIPALES:\\\\n\")\n",
    "            f.write(f\"Promedio: {stats['mean']:,.0f}\\\\n\")\n",
    "            f.write(f\"Mediana: {stats['50%']:,.0f}\\\\n\")\n",
    "            f.write(f\"MÃ­nimo: {stats['min']:,.0f}\\\\n\")\n",
    "            f.write(f\"MÃ¡ximo: {stats['max']:,.0f}\\\\n\")\n",
    "            f.write(f\"DesviaciÃ³n estÃ¡ndar: {stats['std']:,.0f}\\\\n\")\n",
    "        \n",
    "        print(f\"âœ… Resumen estadÃ­stico guardado en: {summary_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error al guardar archivos: {str(e)}\")\n",
    "\n",
    "# Ejemplo de uso modular profesional\n",
    "print(\"\\\\nğŸ”§ EJEMPLO DE USO MODULAR:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "code_example = '''\n",
    "# Ejemplo de uso profesional de los mÃ³dulos\n",
    "\n",
    "# 1. Importar mÃ³dulos del proyecto\n",
    "from etl.data_processor import LabourForceProcessor\n",
    "from visualization.charts import LabourForceCharts\n",
    "from utils.validators import LabourForceValidator\n",
    "\n",
    "# 2. Crear pipeline de procesamiento\n",
    "processor = LabourForceProcessor()\n",
    "\n",
    "# 3. Procesar datos con validaciÃ³n\n",
    "raw_data = processor.extractor.extract(\"data/raw/labour_force.csv\")\n",
    "clean_data = processor.transformer.transform(raw_data)\n",
    "processor.loader.load(clean_data, \"data/processed/labour_force_clean.csv\")\n",
    "\n",
    "# 4. Crear visualizaciones profesionales\n",
    "charts = LabourForceCharts()\n",
    "fig = charts.plot_time_series(clean_data, save_path=\"outputs/time_series.png\")\n",
    "dashboard = charts.create_interactive_dashboard(clean_data)\n",
    "\n",
    "# 5. Validar calidad de datos\n",
    "validator = LabourForceValidator()\n",
    "validation_results = validator.validate_labour_force_data(clean_data)\n",
    "report = validator.get_validation_report()\n",
    "'''\n",
    "\n",
    "print(code_example)\n",
    "\n",
    "print(\"ğŸ¯ VENTAJAS DE LA ARQUITECTURA MODULAR:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "advantages = [\n",
    "    \"ğŸ”„ ReutilizaciÃ³n: Los mÃ³dulos pueden ser reutilizados en otros proyectos\",\n",
    "    \"ğŸ§ª Testabilidad: Cada mÃ³dulo puede ser testeado independientemente\", \n",
    "    \"ğŸ”§ Mantenibilidad: Cambios localizados no afectan todo el sistema\",\n",
    "    \"ğŸ“ˆ Escalabilidad: FÃ¡cil aÃ±adir nuevas funcionalidades\",\n",
    "    \"ğŸ‘¥ ColaboraciÃ³n: MÃºltiples desarrolladores pueden trabajar en paralelo\",\n",
    "    \"ğŸ“š DocumentaciÃ³n: Cada mÃ³dulo estÃ¡ auto-documentado\",\n",
    "    \"ğŸ¨ Flexibilidad: FÃ¡cil intercambiar implementaciones\",\n",
    "    \"ğŸš€ Productividad: Menos cÃ³digo duplicado, mÃ¡s eficiencia\"\n",
    "]\n",
    "\n",
    "for advantage in advantages:\n",
    "    print(f\"   {advantage}\")\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*60)\n",
    "\n",
    "# InformaciÃ³n del proyecto para stakeholders\n",
    "print(\"\\\\nğŸ“‹ INFORMACIÃ“N PARA STAKEHOLDERS:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "stakeholder_info = [\n",
    "    (\"ğŸ¯ Objetivo del Proyecto\", \"AnÃ¡lisis avanzado de la Fuerza de Trabajo en Chile usando datos oficiales del INE\"),\n",
    "    (\"ğŸ“Š MetodologÃ­a\", \"Data Science con Clean Code y principios SOLID\"),\n",
    "    (\"ğŸ” AnÃ¡lisis Incluidos\", \"Exploratorio, temporal, regional, por gÃ©nero, predictivo\"),\n",
    "    (\"ğŸ“ˆ Visualizaciones\", \"GrÃ¡ficos ejecutivos, dashboards interactivos, mapas\"),\n",
    "    (\"ğŸ¤– Modelos\", \"Predictivos para proyecciones futuras\"),\n",
    "    (\"ğŸ“š DocumentaciÃ³n\", \"Completa y profesional para reproducibilidad\"),\n",
    "    (\"ğŸ”§ Arquitectura\", \"Modular y escalable para mantenimiento\"),\n",
    "    (\"ğŸ“„ Entregables\", \"CÃ³digo, datos procesados, visualizaciones, insights\")\n",
    "]\n",
    "\n",
    "for title, description in stakeholder_info:\n",
    "    print(f\"{title}: {description}\")\n",
    "\n",
    "print(f\"\\\\nğŸ† RESULTADO: Repositorio profesional evidenciando capacidades senior en Data Science\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a60eefd",
   "metadata": {},
   "source": [
    "## ğŸ¯ Conclusiones y PrÃ³ximos Pasos\n",
    "\n",
    "### ğŸ“Š Resumen Ejecutivo\n",
    "\n",
    "Este anÃ¡lisis exploratorio de la Fuerza de Trabajo en Chile demuestra la aplicaciÃ³n de metodologÃ­as avanzadas de Data Science siguiendo las mejores prÃ¡cticas de la industria:\n",
    "\n",
    "### âœ… Logros Principales\n",
    "\n",
    "1. **AnÃ¡lisis Completo**: ExploraciÃ³n exhaustiva de datos del INE con validaciÃ³n de calidad\n",
    "2. **Arquitectura Profesional**: ImplementaciÃ³n de Clean Code y principios SOLID\n",
    "3. **Visualizaciones Ejecutivas**: GrÃ¡ficos profesionales y dashboards interactivos\n",
    "4. **Insights EconÃ³micos**: IdentificaciÃ³n de patrones y tendencias clave\n",
    "5. **Modelos Predictivos**: Proyecciones basadas en datos histÃ³ricos\n",
    "6. **DocumentaciÃ³n Completa**: CÃ³digo reproducible y bien documentado\n",
    "\n",
    "### ğŸš€ PrÃ³ximos Pasos\n",
    "\n",
    "1. **ExpansiÃ³n de Datos**: Integrar mÃ¡s fuentes del INE (ocupaciÃ³n, desocupaciÃ³n, etc.)\n",
    "2. **Dashboard Web**: Crear aplicaciÃ³n web interactiva con Streamlit/Dash\n",
    "3. **Machine Learning Avanzado**: Implementar modelos de forecasting mÃ¡s sofisticados\n",
    "4. **API de Datos**: Desarrollar API REST para consultas automÃ¡ticas\n",
    "5. **AutomatizaciÃ³n**: Crear pipelines automÃ¡ticos de actualizaciÃ³n de datos\n",
    "6. **AnÃ¡lisis Sectorial**: Incluir anÃ¡lisis por sectores econÃ³micos\n",
    "7. **GeolocalizaciÃ³n**: AÃ±adir mapas interactivos regionales\n",
    "\n",
    "### ğŸ† Valor del Proyecto\n",
    "\n",
    "Este repositorio evidencia capacidades **senior en Data Science** a travÃ©s de:\n",
    "- CÃ³digo limpio y arquitectura escalable\n",
    "- MetodologÃ­a cientÃ­fica rigurosa\n",
    "- Visualizaciones de calidad ejecutiva\n",
    "- DocumentaciÃ³n profesional completa\n",
    "- AplicaciÃ³n de mejores prÃ¡cticas de la industria\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ“§ Contacto**: Bruno San Martin | **ğŸ”— GitHub**: @SanMaBruno | **ğŸ“… Fecha**: Julio 2025"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
